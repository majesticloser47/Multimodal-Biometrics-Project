{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6e58c6",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853f3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import constants\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.cm as cm\n",
    "import scipy.io as sp\n",
    "import pprint as pp\n",
    "from scipy.signal import welch\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c2fe3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=300, suppress=True)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4bc15",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2b9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some calculations\n",
    "# for the dataset, 10 seconds = 1280 frames. 1 second = 128 frames. We can sue this to find the number of seconds that were taken to record the signature.\n",
    "\n",
    "recording_samp_rate = 128 # per second\n",
    "per_phase_frames = 1280 # seconds\n",
    "max_seq_len_for_data = 3000 # frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ee960",
   "metadata": {},
   "source": [
    "# Fetching raw data (Sign + EEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358fbdd",
   "metadata": {},
   "source": [
    "## Processing raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6301463",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "dataset_path = os.getenv('DATASET_PATH')\n",
    "\n",
    "def get_dataset_files_and_user_ids(data_category = constants.GENUINE, data_type = constants.TRAIN):\n",
    "    user_ids = []\n",
    "    labels = []\n",
    "    files_csv = []\n",
    "    files_mat = []\n",
    "\n",
    "    # Get training and testing data\n",
    "    # data_split = pd.read_csv(os.path.join(dataset_path, \"Identification_split.csv\"))\n",
    "    # files_for_task = list(data_split[data_split.set == data_type].filename)\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        if os.path.basename(root) == constants.GENUINE == data_category:\n",
    "            for file in files:\n",
    "                # if file.endswith('.mat') and file in files_for_task:\n",
    "                if file.endswith('.mat'):\n",
    "                    files_mat.append(os.path.join(root, file))\n",
    "                    labels.append(data_category)\n",
    "        elif os.path.basename(root) == constants.FORGED == data_category:\n",
    "            for file in files:\n",
    "                # if file.endswith('.mat') and file in files_for_task:\n",
    "                if file.endswith('.mat'):\n",
    "                    files_mat.append(os.path.join(root, file))\n",
    "                    labels.append(data_category)\n",
    "        if os.path.basename(root) != constants.GENUINE and os.path.basename(root) != constants.FORGED and os.path.basename(root) != 'SignEEGv1.0':\n",
    "            user_ids.append(os.path.basename(root))\n",
    "        \n",
    "    # files_mat = sorted(files_mat, key=lambda x: int(x.split('_')[3]))\n",
    "    # files_mat = [files_mat, [data_category for _ in files_mat]]\n",
    "    return files_mat, user_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4322f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25db097d",
   "metadata": {},
   "source": [
    "## For debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82928b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All files\n",
    "\n",
    "# files_mat_genuine, user_ids_genuine, genuine_labels = get_dataset_files_and_user_ids(data_category=constants.GENUINE)\n",
    "# files_mat_forged, user_ids_forged, forged_labels = get_dataset_files_and_user_ids(data_category=constants.FORGED)\n",
    "\n",
    "# files_mat_genuine.extend(files_mat_forged)\n",
    "# files_mat_appended = files_mat_genuine\n",
    "# genuine_labels.extend(forged_labels)\n",
    "# labels_appended = genuine_labels\n",
    "\n",
    "# sign_data_dict, eeg_data_dict, labels = get_sig_eeg_data_dicts()\n",
    "\n",
    "# # print(labels_appended)\n",
    "# # # print(len(labels_appended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# files_all = np.array(files_mat_appended)\n",
    "# labels_all = np.array(labels_appended)\n",
    "\n",
    "# indices = np.arange(len(files_all))\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# files_mat_appended = files_all[indices]\n",
    "# labels_appended = labels_all[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ad3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(len(files_mat_appended))\n",
    "# to_check = sp.loadmat(files_mat_appended[0])\n",
    "# print(\"User ID: \", to_check['subject']['SubjectID'])\n",
    "# print(\"Sign Data: \", to_check['subject']['SignWacom'][0][0].shape)\n",
    "# print(\"EEG Data: \", to_check['subject']['ICA_EEG'][0][0].shape)\n",
    "# sign_data_test = to_check['subject']['SignWacom'][0][0]\n",
    "# eeg_data_test = to_check['subject']['ICA_EEG'][0][0]\n",
    "\n",
    "# # np.delete(sign_data_test, 0, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0832875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sign_data_test.shape)\n",
    "# eeg_data_test = eeg_data_test.T\n",
    "# total_frames_eeg = eeg_data_test.shape[0]\n",
    "# roi_frames_start = -(total_frames_eeg % per_phase_frames)\n",
    "# print(eeg_data_test[roi_frames_start:].shape)\n",
    "# # print(eeg_data_test[(total_frames_eeg % per_phase_frames) - 1 :, :].T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small note: np.delete(axis = 1) will delete a column, axis = 0 will delete a row. be careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc28d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_sign_data = np.delete(sign_data_test, 0, axis = 1)\n",
    "# print(normalized_sign_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69bfe1d",
   "metadata": {},
   "source": [
    "## Getting list of sign data, eeg data and label for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c4f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_eeg_raw_data(mat_files, labels):\n",
    "    raw_data_list = []\n",
    "    for mat_file, label in zip(mat_files, labels):\n",
    "        mat_content = sp.loadmat(mat_file)\n",
    "        user_id = str(mat_content['subject']['SubjectID'][0][0][0])\n",
    "        sig_data = mat_content['subject']['SignWacom'][0][0]\n",
    "        eeg_ica_data = mat_content['subject']['ICA_EEG'][0][0].T\n",
    "        # if user_id not in user_ids_master_list:\n",
    "        #     user_ids_master_list.append(user_id)\n",
    "        # removing unwanted columns from sign data\n",
    "        sig_data = torch.from_numpy(np.delete(sig_data, 0, axis=1)).to(dtype=torch.float32)\n",
    "        \n",
    "        # getting part of eeg data during which signature was recorded (ROI)\n",
    "        roi_frames_start = -(eeg_ica_data.shape[0] % per_phase_frames) if per_phase_frames > 0 else 0\n",
    "        eeg_ica_data = torch.from_numpy(eeg_ica_data[roi_frames_start:]).to(dtype=torch.float32)\n",
    "\n",
    "        if sig_data.shape[0] > max_seq_len_for_data:\n",
    "            # print(\"Caught you!!!\")\n",
    "            # print(\"User ID: \", user_id)\n",
    "            # print(\"File: \", mat_file)  \n",
    "            continue # Skip these files because it's too long, outlier\n",
    "        raw_data_list.append({\n",
    "            'sign_data': sig_data,\n",
    "            'eeg_data': eeg_ica_data,\n",
    "            'user_id': user_id,\n",
    "            'label': 1 if label == constants.GENUINE else 0,\n",
    "            'file': mat_file\n",
    "        })\n",
    "\n",
    "    return raw_data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d478c1",
   "metadata": {},
   "source": [
    "# Sign Data Classification Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb8429",
   "metadata": {},
   "source": [
    "## Sign data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a2b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sign_data_dict(sign_data):\n",
    "\n",
    "    mean = torch.mean(sign_data[:, 2:], dim=0)\n",
    "    std = torch.std(sign_data[:, 2:], dim=0)\n",
    "    std = torch.where(std == 0, torch.tensor(1.0, dtype=torch.float32), std)\n",
    "    normalized = (sign_data[:, 2:] - mean) / std\n",
    "    normalized = torch.cat([sign_data[:, 0:2], normalized], dim=1).to(dtype=torch.float32)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b972dd",
   "metadata": {},
   "source": [
    "## Sign Data Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eadc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sign_data_features(sign_data):\n",
    "    normalized_sign_data = normalize_sign_data_dict(sign_data)\n",
    "    x = sign_data[:, 2]\n",
    "    y = sign_data[:, 3]\n",
    "\n",
    "    normalized_sign_data = torch.tensor(normalized_sign_data, dtype=torch.float32)\n",
    "    norm_x = normalized_sign_data[:, 2]\n",
    "    norm_y = normalized_sign_data[:, 3]\n",
    "    vx = torch.gradient(norm_x)[0]\n",
    "    vy = torch.gradient(norm_y)[0]\n",
    "    velocity = torch.sqrt(vx**2 + vy**2)\n",
    "    ax = torch.gradient(vx)[0]\n",
    "    ay = torch.gradient(vy)[0]\n",
    "    acceleration = torch.sqrt(ax**2 + ay**2)\n",
    "    \n",
    "    avg_vx = torch.mean(vx)\n",
    "    avg_vy = torch.mean(vy)\n",
    "    avg_ax = torch.mean(ax)\n",
    "    avg_ay = torch.mean(ay)\n",
    "    \n",
    "    # log curvature radius\n",
    "    dt = 1\n",
    "    dx = torch.gradient(norm_x, spacing=(dt,))[0]\n",
    "    dy = torch.gradient(norm_y, spacing=(dt,))[0]\n",
    "    v_t = torch.sqrt(dx ** 2 + dy ** 2)\n",
    "    v_t = torch.where(v_t == 0, torch.tensor(1e-10, dtype=v_t.dtype), v_t)\n",
    "    theta = torch.atan2(dy, dx)\n",
    "    dtheta = torch.gradient(theta, spacing=(dt,))[0]\n",
    "    dtheta = torch.where(dtheta == 0, torch.tensor(1e-10, dtype=dtheta.dtype), dtheta)\n",
    "    log_curv_radius = torch.log(torch.abs(v_t / dtheta) + 1e-10)\n",
    "    # print(\"Log Curve Radius shape: \", log_curv_radius.shape)\n",
    "    # getting static features\n",
    "    pendown_frames = normalized_sign_data[:, 1] == 1\n",
    "    num_strokes = torch.unique(normalized_sign_data[pendown_frames][:, 0]).shape[0]\n",
    "    x_down = normalized_sign_data[pendown_frames][:, 2]\n",
    "    y_down = normalized_sign_data[pendown_frames][:, 3]\n",
    "    sign_centroid = torch.tensor([torch.mean(x_down), torch.mean(y_down)], dtype=torch.float32)\n",
    "    if y_down.shape[0] > 0:\n",
    "        sign_height = torch.max(y_down) - torch.min(y_down)\n",
    "    else:\n",
    "        sign_height = 0\n",
    "    if x_down.shape[0] > 0:\n",
    "        sign_width = torch.max(x_down) - torch.min(x_down)\n",
    "    else:\n",
    "        sign_width = 0\n",
    "    height_width_ratio = sign_height / sign_width if sign_width != 0 else torch.tensor(0.0, dtype=torch.float32)\n",
    "    \n",
    "    pressure = sign_data[pendown_frames][:, 4]\n",
    "    azimuth = sign_data[pendown_frames][:, 5]\n",
    "    altitude = sign_data[pendown_frames][:, 6]\n",
    "    avg_pressure = torch.mean(pressure)\n",
    "    avg_azimuth = torch.mean(azimuth)\n",
    "    avg_altitude = torch.mean(altitude)\n",
    "    max_pressure = torch.max(pressure) if pressure.numel() > 0 else torch.tensor(0.0, dtype=torch.float32)\n",
    "    cls_token = torch.tensor([\n",
    "        num_strokes, sign_height, sign_width, height_width_ratio, sign_centroid[0], sign_centroid[1], avg_pressure, avg_azimuth, avg_altitude, avg_vx, avg_vy, avg_ax, avg_ay, max_pressure], dtype=torch.float32)\n",
    "    sign_data_aug = torch.cat([normalized_sign_data, velocity.unsqueeze(1), acceleration.unsqueeze(1), log_curv_radius.unsqueeze(1)], dim=1)\n",
    "\n",
    "    return sign_data_aug, cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sign_max_seq_len(sign_data):\n",
    "#     max_len = 0\n",
    "#     for user in sign_data.keys():\n",
    "#         for data in sign_data[user]['data']:\n",
    "#             max_len = max(max_len, data.shape[0])\n",
    "#     return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_time_taken_for_each_sign(normalized_eeg_data_dict, sign_features_dict):\n",
    "#     sign_stats = {}\n",
    "#     for user in normalized_eeg_data_dict.keys():\n",
    "#         user_eeg_data = normalized_eeg_data_dict[user]\n",
    "#         eeg_frames_for_sign = user_eeg_data[0].shape[0]\n",
    "#         time_taken_for_sign = eeg_frames_for_sign / recording_samp_rate\n",
    "#         if user not in sign_stats.keys():\n",
    "#             sign_stats[user] = []\n",
    "#         sign_stats[user].append(time_taken_for_sign)\n",
    "#     return sign_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d864783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign_times_for_users = get_time_taken_for_each_sign(normalized_eeg_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75128f7a",
   "metadata": {},
   "source": [
    "## Prepare sign dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb40603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_attention_tokens_and_padding(data, max_len):\n",
    "    seq_len, feat_dim = data.shape\n",
    "    pad_width = (0, max_len - seq_len)\n",
    "    padded_data = torch.nn.functional.pad(data, (0, 0, 0, pad_width[1]), mode='constant', value=0)\n",
    "    attention_mask = torch.zeros(max_len + 1, dtype=torch.float32)\n",
    "    attention_mask[:seq_len + 1] = 1  # +1 for cls_token\n",
    "    return padded_data, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_sign_dataset_with_all_parts(final_sign_data):\n",
    "#     final_sign_dataset = {}\n",
    "#     final_sign_dataset['cls_tokens'] = []\n",
    "#     final_sign_dataset['data'] = []\n",
    "#     final_sign_dataset['attention_masks'] = []\n",
    "#     final_sign_dataset['labels'] = []\n",
    "#     for user in final_sign_data:\n",
    "#         final_sign_dataset['cls_tokens'].extend(final_sign_data[user]['cls_tokens'])\n",
    "#         final_sign_dataset['data'].extend(final_sign_data[user]['data'])\n",
    "#         final_sign_dataset['attention_masks'].extend(final_sign_data[user]['attention_masks'])\n",
    "#         final_sign_dataset['labels'].extend(final_sign_data[user]['labels'])\n",
    "#     final_sign_dataset['cls_tokens'] = torch.stack(final_sign_dataset['cls_tokens'])\n",
    "#     final_sign_dataset['data'] = torch.stack(final_sign_dataset['data'])\n",
    "#     final_sign_dataset['attention_masks'] = torch.stack(final_sign_dataset['attention_masks'])\n",
    "#     final_sign_dataset['labels'] = torch.stack(final_sign_dataset['labels'])\n",
    "#     return final_sign_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For single data\n",
    "\n",
    "# sign_dict_sample, eeg_dict_sample, label = get_sig_eeg_data_dicts([files_mat_appended[0]], [1])\n",
    "# # print(sign_dict_sample['000000000200894'][0].shape)\n",
    "# normalized_sign_data_sample = normalize_sign_data_dict(sign_dict_sample)\n",
    "# sign_with_features = get_sign_data_features(normalized_sign_data_sample)\n",
    "# # print()\n",
    "# final_sign_data = sign_attach_attention_tokens_and_labels(sign_with_features, label)\n",
    "# final_sign_dataset = prepare_sign_dataset_with_all_parts(final_sign_data)\n",
    "# # print(\"Max len: \", get_sign_max_seq_len(sign_with_features))\n",
    "# # print(final_sign_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33ebbb",
   "metadata": {},
   "source": [
    "## Sign Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedde36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes will be 2 (0 = unauthenticated, 1 = authenticated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d20fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' current data format:\n",
    "\n",
    "Just for reference, so that I don't forget later\n",
    "\n",
    "original data: \n",
    "\n",
    "{\n",
    "    user_id: string,\n",
    "    data: [ [ [], [], [] ... ], [ [], [], [] ... ] ...  ], size: (30 * 1200 * 7) = (num_samples_per_user * time_series_len * num_features)\n",
    "}\n",
    "\n",
    "extracted data:\n",
    "\n",
    "{\n",
    "    cls_tokens: tensor([ , , , , ... ], [ , , , ,  ....], [ , , , , ...] ... ), size: (total_num_samples * num_features),\n",
    "    data: tensor([ [], [], [] ... ], [ [], [], [] ... ] ...), size: (total_num_samples * time_series_len * num_features),\n",
    "    attention_masks: tensor([1, 1, 1, ...], [1, 1,1, ...] ... ), size: (total_num_samples * time_series_len),\n",
    "    labels: tensor([1, 0, 1, 0 ...]), size: (total_num_samples, )\n",
    "}\n",
    "'''\n",
    "\n",
    "class SignatureDataset(Dataset):\n",
    "    def __init__(self, input_data, num_classes):\n",
    "        self.seq_len = input_data['data'][0].shape[0]\n",
    "        self.ts_dim = input_data['data'][0].shape[1]\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.x_ts = input_data['data']\n",
    "        self.cls_token = input_data['cls_tokens']\n",
    "        self.labels = input_data['labels']\n",
    "        self.attention_mask = input_data['attention_masks']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x_ts': self.x_ts[idx],\n",
    "            'cls_token': self.cls_token[idx],\n",
    "            'labels': self.labels[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ebf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len + 1, d_model)\n",
    "        position = torch.arange(0, max_len + 1, dtype = torch.float).unsqueeze(1)\n",
    "        divterm = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Credits to hkproj@github for this as https://github.com/hkproj/pytorch-transformer/blob/main/model.py\n",
    "        pe[:, 0::2] = torch.sin(position * divterm)\n",
    "        pe[:, 1::2] = torch.cos(position * divterm)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014ab58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, cls_dim, d_model, num_classes, num_heads, num_layers, max_seq_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.cls_proj = nn.Linear(cls_dim, d_model)\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dropout = dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        # uncomment for single modality\n",
    "        # self.classifier = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, x_ts, cls_token, attn_mask = None):\n",
    "        x_ts = torch.nan_to_num(x_ts, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        cls_token = torch.nan_to_num(cls_token, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        batch_size, t, feat_dim = x_ts.shape\n",
    "        x_proj = self.input_projection(x_ts)\n",
    "        cls_proj = self.cls_proj(cls_token).unsqueeze(1)\n",
    "        # print(\"x_proj size: \", x_proj.shape)\n",
    "        # print(\"cls_proj size: \", cls_proj.shape)\n",
    "        x = torch.cat([cls_proj, x_proj], dim=1)\n",
    "        # print(\"x_proj and cls_proj concatenated size: \", x.shape)\n",
    "        x = x + self.positional_encoding(x)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask == 0 # True = ignore the value, False = include it!!!!!!!!!!\n",
    "            # cls_mask = torch.zeros((batch_size, 1), dtype=torch.bool, device=attn_mask.device)\n",
    "            full_mask = torch.cat([attn_mask], dim=1)  # [batch_size, t+1]\n",
    "        else:\n",
    "            full_mask = None\n",
    "        # print(\"Mask shape: \", full_mask.shape)\n",
    "        x = self.transformer(x, src_key_padding_mask=full_mask)\n",
    "        cls_output = x[:, 0, :]\n",
    "        # uncomment for single modality transformer\n",
    "        # logits = self.classifier(cls_output)\n",
    "        # return logits\n",
    "\n",
    "        # uncomment for multimodal transformer\n",
    "        return cls_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbdd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_training_loop(model, dataloader, optimizer, loss_fn, device, num_epochs = 10):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    model_path = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for batch in dataloader:\n",
    "            x_ts = batch['x_ts'].to(device)\n",
    "            cls_token = batch['cls_token'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_ts, cls_token, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "              print(\"NaN loss detected!\")\n",
    "              print(\"Labels:\", labels)\n",
    "              print(\"Logits:\", logits)\n",
    "              break\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # added to check for exploding gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x_ts.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += x_ts.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "        accuracy = total_correct / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f} - Acc: {accuracy:.4f}\")\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, f\"model{datetime.now().strftime('%m%d%Y-%H%M%S')}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7536eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For all data\n",
    "\n",
    "# files_mat_genuine, user_ids_genuine, genuine_labels = get_dataset_files_and_user_ids(data_category=constants.GENUINE)\n",
    "# files_mat_forged, user_ids_forged, forged_labels = get_dataset_files_and_user_ids(data_category=constants.FORGED)\n",
    "\n",
    "# files_mat_genuine.extend(files_mat_forged)\n",
    "# files_mat_appended = files_mat_genuine\n",
    "# genuine_labels.extend(forged_labels)\n",
    "# labels_appended = genuine_labels\n",
    "\n",
    "# # shuffling to prevent overfitting\n",
    "# files_all = np.array(files_mat_appended)\n",
    "# labels_all = np.array(labels_appended)\n",
    "\n",
    "# indices = np.arange(len(files_all))\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# files_mat_appended = files_all[indices]\n",
    "# labels_appended = labels_all[indices]\n",
    "\n",
    "# sign_data_dict, eeg_data_dict, user_labels = get_sig_eeg_raw_data(files_mat_appended, labels_appended)\n",
    "# normalized_sign_data_dict = normalize_sign_data_dict(sign_data_dict)\n",
    "# sign_data_with_features = get_sign_data_features(sign_data_dict) # changing the get features function by shifting normalization logic into it\n",
    "# final_signature_data = sign_attach_attention_tokens_and_labels(sign_data_with_features, user_labels)\n",
    "# final_sign_dataset_for_all_users = prepare_sign_dataset_with_all_parts(final_signature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b8f2a",
   "metadata": {},
   "source": [
    "## Sign Data Feed to Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = final_sign_dataset_for_all_users\n",
    "ts_dim = input_data['data'][0].size(1)\n",
    "cls_dim = input_data['cls_tokens'][0].size(0)\n",
    "d_model = 64\n",
    "num_classes = 2\n",
    "seq_len = get_sign_max_seq_len(sign_data_with_features)\n",
    "batch_size = 8\n",
    "\n",
    "dataset = SignatureDataset(input_data, num_classes)\n",
    "# dataset.__getitem__(0)['x_ts'].shape\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "sign_model = SignatureTransformer(input_dim=ts_dim, cls_dim=cls_dim, d_model=d_model, num_classes=num_classes, num_heads=4, num_layers=4, max_seq_len=seq_len)\n",
    "optimizer = optim.Adam(sign_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "sign_training_loop(sign_model, dataloader, optimizer, loss_fn, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a51e3b",
   "metadata": {},
   "source": [
    "## Sign - Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_signature_data = sign_attach_attention_tokens_and_labels(sign_data_with_features, user_labels)\n",
    "# final_sign_dataset_for_all_users = prepare_sign_dataset_with_all_parts(final_signature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(final_signature_data['002108410100044']['cls_tokens']))\n",
    "# print(len(final_signature_data['002108410100044']['data']))\n",
    "# print(len(final_signature_data['002108410100044']['attention_masks']))\n",
    "\n",
    "# print(final_signature_data['002108410100044']['cls_tokens'][0].shape)\n",
    "# print(final_signature_data['002108410100044']['data'][0].shape)\n",
    "# print(final_signature_data['002108410100044']['attention_masks'][0].shape)\n",
    "\n",
    "\n",
    "print(len(final_sign_dataset_for_all_users['cls_tokens']))\n",
    "print(len(final_sign_dataset_for_all_users['data']))\n",
    "print(len(final_sign_dataset_for_all_users['attention_masks']))\n",
    "print(len(final_sign_dataset_for_all_users['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec3dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(len(final_sign_dataset_for_all_users['cls_tokens']))\n",
    "# print(len(final_sign_dataset_for_all_users['data']))\n",
    "# print(len(final_sign_dataset_for_all_users['attention_masks']))\n",
    "\n",
    "# print(final_sign_dataset_for_all_users['cls_tokens'][0].shape)\n",
    "# print(final_sign_dataset_for_all_users['data'][0].shape)\n",
    "# print(final_sign_dataset_for_all_users['attention_masks'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93df9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(normalized_sign_data_dict['000000000200894'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_sign_data_dict = normalize_sign_data_dict(sign_data_dict)\n",
    "# # print(sign_data_dict['000000000200894'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4fc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(sign_times_for_users))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab25fe",
   "metadata": {},
   "source": [
    "# EEG Data Classification Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fe1a5",
   "metadata": {},
   "source": [
    "## EEG Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_eeg_data_dict(eeg_data_dict):\n",
    "    normalized_eeg_data_dict = {}\n",
    "    for user_id, eeg_list in eeg_data_dict.items():\n",
    "        normalized_eeg_data_dict[user_id] = []\n",
    "        for eeg_data in eeg_list:\n",
    "            mean = eeg_data.mean(dim=0, keepdim=True)\n",
    "            std = eeg_data.std(dim=0, keepdim=True)\n",
    "            std = torch.where(std == 0, torch.tensor(1.0, dtype=std.dtype, device=std.device), std)\n",
    "            normalized = (eeg_data - mean) / std\n",
    "            normalized_eeg_data_dict[user_id].append(normalized)\n",
    "    return normalized_eeg_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c86c80",
   "metadata": {},
   "source": [
    "## EEG Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c6685fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_difference_mean_for_signal(input_signal, n):\n",
    "    input_signal = torch.as_tensor(input_signal)\n",
    "    diff = torch.abs(input_signal[n:] - input_signal[:-n])\n",
    "    res = torch.sum(diff) / (input_signal.shape[0] - n)\n",
    "    return res\n",
    "\n",
    "def normalize_for_eeg_related_data(data):\n",
    "    data = torch.as_tensor(data, dtype=torch.float32)\n",
    "    mean = torch.mean(data, dim=0)\n",
    "    std = torch.std(data, dim=0)\n",
    "    std = torch.where(std == 0, torch.tensor(1.0, dtype=std.dtype, device=std.device), std)\n",
    "    norm = (data - mean) / std\n",
    "    return norm\n",
    "\n",
    "def get_eeg_data_features(eeg_data, fs=recording_samp_rate):\n",
    "\n",
    "    signal_mean = torch.mean(eeg_data)\n",
    "    signal_std = torch.std(eeg_data)\n",
    "\n",
    "    first_difference_sample_mean_absolute_difference_raw_signal = get_nth_difference_mean_for_signal(eeg_data, 1)\n",
    "    second_difference_sample_mean_absolute_difference_raw_signal = get_nth_difference_mean_for_signal(eeg_data, 2)\n",
    "\n",
    "    normalized_signal = normalize_for_eeg_related_data(eeg_data)\n",
    "    first_difference_sample_mean_absolute_difference_normalized_signal = get_nth_difference_mean_for_signal(normalized_signal, 1)\n",
    "    second_difference_sample_mean_absolute_difference_normalized_signal = get_nth_difference_mean_for_signal(normalized_signal, 2)\n",
    "    fw_powers = []\n",
    "    eeg_data = torch.as_tensor(eeg_data, dtype=torch.float32)\n",
    "    for ch in range(eeg_data.shape[1]):\n",
    "        # Welch returns numpy arrays, so convert to torch\n",
    "        f, Pxx = welch(eeg_data[:, ch].cpu().numpy(), fs=fs)\n",
    "        f = torch.from_numpy(f).to(eeg_data.device)\n",
    "        Pxx = torch.from_numpy(Pxx).to(eeg_data.device)\n",
    "        fw_power = torch.sum(f * Pxx) / torch.sum(Pxx) if torch.sum(Pxx) > 0 else torch.tensor(0.0, device=eeg_data.device)\n",
    "        fw_powers.append(fw_power)\n",
    "    fw_power_arr = torch.stack(fw_powers).unsqueeze(0)\n",
    "    # cls_token = torch.cat([signal_mean, signal_std, first_difference_sample_mean_absolute_difference_raw_signal, second_difference_sample_mean_absolute_difference_raw_signal, first_difference_sample_mean_absolute_difference_normalized_signal, second_difference_sample_mean_absolute_difference_normalized_signal])\n",
    "    # cls_token = torch.stack(fw_power_arr)\n",
    "    # Combine all extracted features into a 1D tensor (flattened)\n",
    "    features = [signal_mean, signal_std, first_difference_sample_mean_absolute_difference_raw_signal, second_difference_sample_mean_absolute_difference_raw_signal, first_difference_sample_mean_absolute_difference_normalized_signal, second_difference_sample_mean_absolute_difference_normalized_signal]\n",
    "    features.extend(fw_power_arr.squeeze(0).tolist())\n",
    "    cls_token = torch.tensor(features, dtype=torch.float32)\n",
    "    return normalized_signal, cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_eeg_max_seq_len(eeg_feature_data):\n",
    "#     max_len = 0\n",
    "#     for user in eeg_feature_data:\n",
    "#         for eeg_data in eeg_feature_data[user]['data']:\n",
    "#             max_len = max(max_len, eeg_data.shape[0])\n",
    "#     return max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91273bfa",
   "metadata": {},
   "source": [
    "## EEG Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeg_attach_attention_tokens_and_labels(eeg_feature_data, labels_dict, user_ids_list):\n",
    "    max_seq_len = get_eeg_max_seq_len(eeg_feature_data)\n",
    "    print(\"Max seq len: \", max_seq_len)\n",
    "    final_eeg_data = {}\n",
    "    for user in user_ids_list:\n",
    "        final_eeg_data[user] = {}\n",
    "        final_eeg_data[user]['data'] = []\n",
    "        final_eeg_data[user]['cls_tokens'] = eeg_feature_data[user]['cls_tokens']\n",
    "        final_eeg_data[user]['attention_masks'] = []\n",
    "        final_eeg_data[user]['labels'] = labels_dict[user]\n",
    "        for data in eeg_feature_data[user]['data']:\n",
    "            seq_len = data.shape[0]\n",
    "            pad_len = max_seq_len - seq_len\n",
    "            padded_data = torch.nn.functional.pad(data, (0, 0, 0, pad_len), mode='constant', value=0)\n",
    "            attention_mask = torch.zeros(max_seq_len+1, dtype=torch.float32)\n",
    "            attention_mask[:seq_len + 1] = 1\n",
    "            final_eeg_data[user]['data'].append(padded_data)\n",
    "            final_eeg_data[user]['attention_masks'].append(attention_mask)\n",
    "    return final_eeg_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c448265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_eeg_dataset_with_all_parts(final_eeg_data):\n",
    "#     final_eeg_dataset = {}\n",
    "#     final_eeg_dataset['cls_tokens'] = []\n",
    "#     final_eeg_dataset['data'] = []\n",
    "#     final_eeg_dataset['attention_masks'] = []\n",
    "#     final_eeg_dataset['labels'] = []\n",
    "#     for user in final_eeg_data:\n",
    "#         final_eeg_dataset['cls_tokens'].extend(final_eeg_data[user]['cls_tokens'])\n",
    "#         final_eeg_dataset['data'].extend(final_eeg_data[user]['data'])\n",
    "#         final_eeg_dataset['attention_masks'].extend(final_eeg_data[user]['attention_masks'])\n",
    "#         final_eeg_dataset['labels'].extend(final_eeg_data[user]['labels'])\n",
    "#     final_eeg_dataset['cls_tokens'] = torch.stack(final_eeg_dataset['cls_tokens'])\n",
    "#     final_eeg_dataset['data'] = torch.stack(final_eeg_dataset['data'])\n",
    "#     final_eeg_dataset['attention_masks'] = torch.stack(final_eeg_dataset['attention_masks'])\n",
    "#     final_eeg_dataset['labels'] = torch.stack(final_eeg_dataset['labels'])\n",
    "#     return final_eeg_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36432dfd",
   "metadata": {},
   "source": [
    "## EEG Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All EEG Data\n",
    "\n",
    "files_mat_genuine, user_ids_genuine, genuine_labels = get_dataset_files_and_user_ids(data_category=constants.GENUINE)\n",
    "files_mat_forged, user_ids_forged, forged_labels = get_dataset_files_and_user_ids(data_category=constants.FORGED)\n",
    "\n",
    "files_mat_genuine.extend(files_mat_forged)\n",
    "files_mat_appended = files_mat_genuine\n",
    "genuine_labels.extend(forged_labels)\n",
    "labels_appended = genuine_labels\n",
    "\n",
    "# shuffling to prevent overfitting\n",
    "files_all = np.array(files_mat_appended)\n",
    "labels_all = np.array(labels_appended)\n",
    "\n",
    "indices = np.arange(len(files_all))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "files_mat_appended = files_all[indices]\n",
    "labels_appended = labels_all[indices]\n",
    "\n",
    "sign_data_dict, eeg_data_dict, labels = get_sig_eeg_raw_data(files_mat_appended, labels_appended)\n",
    "normalized_eeg_data_dict = normalize_eeg_data_dict(eeg_data_dict)\n",
    "eeg_data_with_features = get_eeg_data_features(normalized_eeg_data_dict)\n",
    "eeg_final_data = eeg_attach_attention_tokens_and_labels(eeg_data_with_features, labels)\n",
    "eeg_final_dataset = prepare_eeg_dataset_with_all_parts(eeg_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = eeg_final_dataset\n",
    "ts_dim = input_data['data'][0].size(1)\n",
    "cls_dim = input_data['cls_tokens'][0].size(0)\n",
    "d_model = 64\n",
    "num_classes = 2\n",
    "seq_len = get_eeg_max_seq_len(eeg_data_with_features)\n",
    "batch_size = 8\n",
    "\n",
    "dataset = SignatureDataset(input_data, num_classes)\n",
    "# dataset.__getitem__(0)['x_ts'].shape\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "eeg_model = SignatureTransformer(input_dim=ts_dim, cls_dim=cls_dim, d_model=d_model, num_classes=num_classes, num_heads=4, num_layers=4, max_seq_len=seq_len)\n",
    "optimizer = optim.Adam(eeg_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "sign_training_loop(eeg_model, dataloader, optimizer, loss_fn, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f624e0",
   "metadata": {},
   "source": [
    "# Sign + EEG Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15581d",
   "metadata": {},
   "source": [
    "## Getting the EEG and Sign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314126b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_29644\\2982074425.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  normalized_sign_data = torch.tensor(normalized_sign_data, dtype=torch.float32)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_29644\\1572319389.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 136, using nperseg = 136\n",
      "  f, Pxx = welch(eeg_data[:, ch].cpu().numpy(), fs=fs)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_29644\\1572319389.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 197, using nperseg = 197\n",
      "  f, Pxx = welch(eeg_data[:, ch].cpu().numpy(), fs=fs)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_29644\\1572319389.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 203, using nperseg = 203\n",
      "  f, Pxx = welch(eeg_data[:, ch].cpu().numpy(), fs=fs)\n"
     ]
    }
   ],
   "source": [
    "files_mat_genuine, user_ids_genuine, genuine_labels = get_dataset_files_and_user_ids(data_category=constants.GENUINE)\n",
    "files_mat_forged, user_ids_forged, forged_labels = get_dataset_files_and_user_ids(data_category=constants.FORGED)\n",
    "\n",
    "files_mat_genuine.extend(files_mat_forged)\n",
    "files_mat_appended = files_mat_genuine\n",
    "genuine_labels.extend(forged_labels)\n",
    "labels_appended = genuine_labels\n",
    "\n",
    "# shuffling to prevent overfitting\n",
    "files_all = np.array(files_mat_appended)\n",
    "labels_all = np.array(labels_appended)\n",
    "\n",
    "indices = np.arange(len(files_all))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "files_mat_appended = files_all[indices]\n",
    "labels_appended = labels_all[indices]\n",
    "\n",
    "raw_data = get_sig_eeg_raw_data(files_mat_appended, labels_appended)\n",
    "for i in range(len(raw_data)):\n",
    "    sign_data_with_features, sign_cls_token = get_sign_data_features(raw_data[i]['sign_data'])\n",
    "    eeg_data_with_features, eeg_cls_token = get_eeg_data_features(raw_data[i]['eeg_data'])\n",
    "    raw_data[i]['sign_data'] = sign_data_with_features\n",
    "    raw_data[i]['sign_cls_token'] = sign_cls_token\n",
    "    raw_data[i]['eeg_data'] = eeg_data_with_features\n",
    "    raw_data[i]['eeg_cls_token'] = eeg_cls_token\n",
    "\n",
    "sign_max_seq_len = max([data['sign_data'].shape[0] for data in raw_data])\n",
    "eeg_max_seq_len = max([data['eeg_data'].shape[0] for data in raw_data])\n",
    "\n",
    "for i in range(len(raw_data)):\n",
    "    sign_data = raw_data[i]['sign_data']\n",
    "    eeg_data = raw_data[i]['eeg_data']\n",
    "    sign_data, sign_attention_mask = attach_attention_tokens_and_padding(sign_data, sign_max_seq_len)\n",
    "    eeg_data, eeg_attention_mask = attach_attention_tokens_and_padding(eeg_data, eeg_max_seq_len)\n",
    "    raw_data[i]['sign_data'] = sign_data\n",
    "    raw_data[i]['eeg_data'] = eeg_data\n",
    "    raw_data[i]['sign_attention_mask'] = sign_attention_mask\n",
    "    raw_data[i]['eeg_attention_mask'] = eeg_attention_mask\n",
    "\n",
    "sign_data = [data['sign_data'] for data in raw_data]\n",
    "eeg_data = [data['eeg_data'] for data in raw_data]\n",
    "sign_attention_masks = [data['sign_attention_mask'] for data in raw_data]\n",
    "eeg_attention_masks = [data['eeg_attention_mask'] for data in raw_data]\n",
    "sign_cls_tokens = [data['sign_cls_token'] for data in raw_data]\n",
    "eeg_cls_tokens = [data['eeg_cls_token'] for data in raw_data]\n",
    "labels = [data['label'] for data in raw_data]\n",
    "files = [data['file'] for data in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "794909ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign data type:  <class 'list'>\n",
      "EEG data type:  <class 'list'>\n",
      "Sign attention mask type:  <class 'torch.Tensor'>\n",
      "EEG attention mask type:  <class 'torch.Tensor'>\n",
      "Sign cls token type:  <class 'torch.Tensor'>\n",
      "EEG cls token type:  <class 'torch.Tensor'>\n",
      "Labels type:  <class 'list'>\n",
      "Files type:  <class 'list'>\n",
      "Sign data shape:  torch.Size([2996, 10])\n",
      "EEG data shape:  torch.Size([1247, 5])\n",
      "Sign attention mask shape:  torch.Size([2997])\n",
      "EEG attention mask shape:  torch.Size([1248])\n",
      "Sign cls token shape:  torch.Size([14])\n",
      "EEG cls token shape:  torch.Size([11])\n",
      "Labels shape:  2042\n",
      "Files shape:  2042\n"
     ]
    }
   ],
   "source": [
    "print(\"Sign data type: \", type(sign_data))\n",
    "print(\"EEG data type: \", type(eeg_data))\n",
    "print(\"Sign attention mask type: \", type(sign_attention_masks[0]))\n",
    "print(\"EEG attention mask type: \", type(eeg_attention_masks[0]))\n",
    "print(\"Sign cls token type: \", type(sign_cls_tokens[0]))\n",
    "print(\"EEG cls token type: \", type(eeg_cls_tokens[0]))\n",
    "print(\"Labels type: \", type(labels))\n",
    "print(\"Files type: \", type(files))\n",
    "\n",
    "\n",
    "print(\"Sign data shape: \", sign_data[0].shape)\n",
    "print(\"EEG data shape: \", eeg_data[0].shape)\n",
    "print(\"Sign attention mask shape: \", sign_attention_masks[0].shape)\n",
    "print(\"EEG attention mask shape: \", eeg_attention_masks[0].shape)\n",
    "print(\"Sign cls token shape: \", sign_cls_tokens[0].shape)\n",
    "print(\"EEG cls token shape: \", eeg_cls_tokens[0].shape)\n",
    "print(\"Labels shape: \", len(labels))\n",
    "print(\"Files shape: \", len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c6a5e",
   "metadata": {},
   "source": [
    "## Redesigning the Transformer model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeec2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureEEGDataset(Dataset):\n",
    "    def __init__(self, input_data, num_classes):\n",
    "        sign_data = input_data['sign_data']\n",
    "        eeg_data = input_data['eeg_data']\n",
    "        sign_attention_masks = input_data['sign_attention_masks']\n",
    "        eeg_attention_masks = input_data['eeg_attention_masks']\n",
    "        sign_cls_tokens = input_data['sign_cls_tokens']\n",
    "        eeg_cls_tokens = input_data['eeg_cls_tokens']\n",
    "        labels = input_data['labels']\n",
    "\n",
    "        # self.sign_x_ts = sign_data['data']\n",
    "        # self.sign_cls_token = sign_data['cls_tokens']\n",
    "        # self.sign_attention_mask = sign_data['attention_masks']\n",
    "        # self.sign_seq_len = sign_data['data'][0].shape[0]\n",
    "        # self.sign_ts_dim = sign_data['data'][0].shape[1]\n",
    "\n",
    "        self.sign_x_ts = sign_data\n",
    "        self.sign_cls_token = sign_cls_tokens\n",
    "        self.sign_attention_mask = sign_attention_masks\n",
    "        self.sign_seq_len = sign_data[0].shape[0]\n",
    "        self.sign_ts_dim = sign_data[0].shape[1]\n",
    "\n",
    "        # self.eeg_x_ts = eeg_data['data']\n",
    "        # self.eeg_cls_token = eeg_data['cls_tokens']\n",
    "        # self.eeg_attention_mask = eeg_data['attention_masks']\n",
    "        # self.eeg_seq_len = eeg_data['data'][0].shape[0]\n",
    "        # self.eeg_ts_dim = eeg_data['data'][0].shape[1]\n",
    "\n",
    "        self.eeg_x_ts = eeg_data\n",
    "        self.eeg_cls_token = eeg_cls_tokens\n",
    "        self.eeg_attention_mask = eeg_attention_masks\n",
    "        self.eeg_seq_len = eeg_data[0].shape[0]\n",
    "        self.eeg_ts_dim = eeg_data[0].shape[1]\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sign_x_ts': self.sign_x_ts[idx],\n",
    "            'sign_cls_token': self.sign_cls_token[idx],\n",
    "            'sign_attention_mask': self.sign_attention_mask[idx],\n",
    "            'eeg_x_ts': self.eeg_x_ts[idx],\n",
    "            'eeg_cls_token': self.eeg_cls_token[idx],\n",
    "            'eeg_attention_mask': self.eeg_attention_mask[idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61162a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len + 1, d_model)\n",
    "        position = torch.arange(0, max_len + 1, dtype = torch.float).unsqueeze(1)\n",
    "        divterm = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Credits to hkproj@github for this as https://github.com/hkproj/pytorch-transformer/blob/main/model.py\n",
    "        pe[:, 0::2] = torch.sin(position * divterm)\n",
    "        pe[:, 1::2] = torch.cos(position * divterm)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d21068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureEEGTransformer(nn.Module):\n",
    "    def __init__(self, sign_input_dim, sign_cls_dim, eeg_input_dim, eeg_cls_dim, d_model, num_classes, num_heads, num_layers, sign_max_seq_len, eeg_max_seq_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        # self.d_model = d_model\n",
    "        # self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        # self.cls_proj = nn.Linear(cls_dim, d_model)\n",
    "        # self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        # encoder_layer = nn.TransformerEncoderLayer(\n",
    "        #     d_model=d_model,\n",
    "        #     nhead=num_heads,\n",
    "        #     dropout = dropout,\n",
    "        #     batch_first=True\n",
    "        # )\n",
    "        # self.transformer = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "        self.sign_transfomer = SignatureTransformer(sign_input_dim, sign_cls_dim, d_model, num_classes, num_heads, num_layers, sign_max_seq_len, dropout)\n",
    "        self.eeg_transformer = SignatureTransformer(eeg_input_dim, eeg_cls_dim, d_model, num_classes, num_heads, num_layers, eeg_max_seq_len, dropout)\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Linear(d_model * 2, d_model), nn.ReLU(), nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attn_mask = None, eeg_attn_mask = None):\n",
    "        # x_ts = torch.nan_to_num(x_ts, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        # cls_token = torch.nan_to_num(cls_token, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        # batch_size, t, feat_dim = x_ts.shape\n",
    "        # x_proj = self.input_projection(x_ts)\n",
    "        # cls_proj = self.cls_proj(cls_token).unsqueeze(1)\n",
    "        # # print(\"x_proj size: \", x_proj.shape)\n",
    "        # # print(\"cls_proj size: \", cls_proj.shape)\n",
    "        # x = torch.cat([cls_proj, x_proj], dim=1)\n",
    "        # # print(\"x_proj and cls_proj concatenated size: \", x.shape)\n",
    "        # x = x + self.positional_encoding(x)\n",
    "\n",
    "        # if attn_mask is not None:\n",
    "        #     attn_mask = attn_mask == 0 # True = ignore the value, False = include it!!!!!!!!!!\n",
    "        #     # cls_mask = torch.zeros((batch_size, 1), dtype=torch.bool, device=attn_mask.device)\n",
    "        #     full_mask = torch.cat([attn_mask], dim=1)  # [batch_size, t+1]\n",
    "        # else:\n",
    "        #     full_mask = None\n",
    "\n",
    "        # x = self.transformer(x, src_key_padding_mask=full_mask)\n",
    "        # cls_output = x[:, 0, :]\n",
    "\n",
    "        sign_cls = self.sign_transfomer(sign_x_ts, sign_cls_token, sign_attn_mask)\n",
    "        eeg_cls = self.eeg_transformer(eeg_x_ts, eeg_cls_token, eeg_attn_mask)\n",
    "        multimodal_cls_output = torch.cat([sign_cls, eeg_cls], dim = 1)\n",
    "\n",
    "        logits = self.classifier(multimodal_cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f88540e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'sign_data': sign_data,\n",
    "    'eeg_data': eeg_data,\n",
    "    'sign_attention_masks': sign_attention_masks,\n",
    "    'eeg_attention_masks': eeg_attention_masks,\n",
    "    'sign_cls_tokens': sign_cls_tokens,\n",
    "    'eeg_cls_tokens': eeg_cls_tokens,\n",
    "    'labels': labels,\n",
    "}\n",
    "num_classes = 2\n",
    "multimodal_dataset = SignatureEEGDataset(input_data, num_classes)\n",
    "batch_size = 8\n",
    "multimodal_dataloader = DataLoader(multimodal_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# sign_ts_dim = input_data['sign_data']['data'][0].size(1)\n",
    "# sign_cls_dim = input_data['sign_data']['cls_tokens'][0].size(0)\n",
    "# sign_seq_len = input_data['sign_data']['data'][0].size(0)\n",
    "# eeg_ts_dim = input_data['eeg_data']['data'][0].size(1)\n",
    "# eeg_cls_dim = input_data['eeg_data']['cls_tokens'][0].size(0)\n",
    "# eeg_seq_len = input_data['eeg_data']['data'][0].size(0)\n",
    "\n",
    "sign_ts_dim = input_data['sign_data'][0].size(1)\n",
    "sign_cls_dim = input_data['sign_cls_tokens'][0].size(0)\n",
    "sign_seq_len = input_data['sign_data'][0].size(0)\n",
    "eeg_ts_dim = input_data['eeg_data'][0].size(1)\n",
    "eeg_cls_dim = input_data['eeg_cls_tokens'][0].size(0)\n",
    "eeg_seq_len = input_data['eeg_data'][0].size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f4f7235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 1, 1, 1, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 256/256 [02:53<00:00,  1.48it/s, Batch Loss=0.7140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6920    0.5122      0.5238      0.5158    0.5198    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 0, 1, 0, 1, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.6998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 2/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6793    0.5744      0.5760      0.6383    0.6055    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 0, 1, 1, 1, 0, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.5305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 3/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6668    0.5926      0.6037      0.5933    0.5985    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 0, 1, 0, 0, 0, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=1.0045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 4/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6549    0.6131      0.6324      0.5828    0.6066    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 1, 0, 1, 0, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.5213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 5/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6475    0.6396      0.6608      0.6077    0.6331    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 1, 0, 1, 0, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.6490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 6/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6363    0.6410      0.6563      0.6268    0.6412    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 0, 0, 0, 0, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.5431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 7/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6255    0.6459      0.6620      0.6297    0.6454    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 1, 1, 0, 1, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.4896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 8/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6157    0.6577      0.6727      0.6450    0.6585    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 1, 0, 1, 1, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.4931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 9/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.6068    0.6729      0.6763      0.6919    0.6840    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 0, 1, 1, 0, 1, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.8635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 10/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5954    0.6836      0.6829      0.7129    0.6976    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 1, 0, 1, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.2135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 11/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5887    0.6851      0.6879      0.7043    0.6960    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 0, 1, 1, 0, 1, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.6911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 12/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5811    0.6993      0.7035      0.7129    0.7082    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 1, 1, 0, 1, 0, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.5146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 13/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5800    0.6978      0.6949      0.7301    0.7121    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 0, 1, 1, 1, 0, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.2633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 14/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5666    0.7189      0.7131      0.7541    0.7330    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([0, 0, 1, 0, 1, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.4035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 15/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5630    0.7209      0.7165      0.7522    0.7339    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 0, 1, 1, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.3545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 16/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5577    0.7189      0.7068      0.7703    0.7372    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 1, 0, 0, 0, 0, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.1966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 17/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5556    0.7199      0.7113      0.7617    0.7357    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 0, 1, 0, 0, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 256/256 [02:51<00:00,  1.49it/s, Batch Loss=0.3866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 18/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5503    0.7228      0.7187      0.7531    0.7355    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 1, 0, 1, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 256/256 [02:52<00:00,  1.48it/s, Batch Loss=0.2743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 19/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5483    0.7209      0.7049      0.7818    0.7414    \n",
      "==================================================\n",
      "\n",
      "torch.Size([8, 2996, 10]) torch.Size([8, 1247, 5]) tensor([1, 0, 0, 1, 1, 0, 0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 256/256 [02:52<00:00,  1.48it/s, Batch Loss=0.1536]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 20/20 Summary:\n",
      "Loss      Accuracy    Precision   Recall    F1-Score  \n",
      "0.5394    0.7341      0.7282      0.7665    0.7469    \n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SignatureEEGTransformer(sign_input_dim=sign_ts_dim, sign_cls_dim=sign_cls_dim, eeg_input_dim=eeg_ts_dim, eeg_cls_dim=eeg_cls_dim, d_model=64, num_classes=2, num_heads=4, num_layers=2, sign_max_seq_len=sign_seq_len, eeg_max_seq_len=eeg_seq_len).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Print a batch to check alignment\n",
    "    for batch in multimodal_dataloader:\n",
    "        print(batch['sign_x_ts'].shape, batch['eeg_x_ts'].shape, batch['labels'])\n",
    "        break\n",
    "\n",
    "    pbar = tqdm(enumerate(multimodal_dataloader), total=len(multimodal_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, batch in pbar:\n",
    "        sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "        sign_cls_token = batch['sign_cls_token'].to(device)\n",
    "        sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "        eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "        eeg_cls_token = batch['eeg_cls_token'].to(device)\n",
    "        eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attention_mask, eeg_attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(all_labels) if all_labels else 0\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "    print(f\"{'Loss':<10}{'Accuracy':<12}{'Precision':<12}{'Recall':<10}{'F1-Score':<10}\")\n",
    "    print(f\"{avg_loss:<10.4f}{acc:<12.4f}{prec:<12.4f}{rec:<10.4f}{f1:<10.4f}\")\n",
    "    print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c68df6",
   "metadata": {},
   "source": [
    "## EEG - Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008dad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For debugging\n",
    "# def get_max_attention_token_len(attention_tokens):\n",
    "#     max_len = 0\n",
    "#     for item in attention_tokens:\n",
    "#         max_len = max(max_len, item.shape[0])\n",
    "#     return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_max_attention_token_len(eeg_final_dataset['attention_masks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(normalized_sign_data_dict['000000000200894'][0])\n",
    "# # print(normalized_eeg_data_dict['000000000200894'][0])\n",
    "# # print(user_labels['000000000200894'][0])\n",
    "# eeg_data = get_eeg_data_features(eeg_data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign_data_dict, eeg_data_dict, labels = get_sig_eeg_data_dicts(files_mat_appended, labels_appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single data\n",
    "# user_id = '002108410100048'\n",
    "# single_eeg_data = {}\n",
    "# single_eeg_data[user_id] = eeg_data_dict[user_id]\n",
    "\n",
    "# normalized_eeg_data_dict = normalize_eeg_data_dict(single_eeg_data)\n",
    "# eeg_data_with_features = get_eeg_data_features(normalized_eeg_data_dict)\n",
    "# eeg_final_data = eeg_attach_attention_tokens_and_labels(eeg_data_with_features, labels)\n",
    "# eeg_final_dataset = prepare_eeg_dataset_with_all_parts(eeg_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c911b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4657fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(eeg_data_with_features['002108410100048']['data'][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec789b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = get_eeg_max_seq_len(eeg_data_with_features)\n",
    "# print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab796cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(eeg_final_data['002108410100048']['labels'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1eaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(eeg_final_dataset['labels'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff475923",
   "metadata": {},
   "source": [
    "# Misc - Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x1 = torch.tensor([[1.0, 2.0],\n",
    "#                    [3.0, 1.0],\n",
    "#                    [0.0, 0.0]])\n",
    "\n",
    "# x2 = torch.tensor([[2.0, 1.0],\n",
    "#                    [0.0, 3.0],\n",
    "#                    [1.0, 1.0]])\n",
    "# distances = F.pairwise_distance(x1, x2)\n",
    "\n",
    "# print(\"Pairwise distances:\", distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c042f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the paper: The instances corresponding to the last EEG activity for each subject were interpolated to match the length of the longest genuine instance for that subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# class SignatureEEGDataset(Dataset):\n",
    "#     def __init__(self, signature_data, eeg_data, labels):\n",
    "\n",
    "#         self.samples = []\n",
    "        \n",
    "#         # Combine the data\n",
    "#         for user_id in signature_data:\n",
    "#             # Ensure we have matching signature, EEG, and label entries\n",
    "#             n_samples = len(signature_data[user_id])\n",
    "#             for i in range(n_samples):\n",
    "#                 signature = signature_data[user_id][i]\n",
    "#                 eeg = eeg_data[user_id][i]\n",
    "#                 label = labels[user_id][i]\n",
    "                \n",
    "#                 # Convert to tensors\n",
    "#                 signature_tensor = torch.FloatTensor(signature)\n",
    "#                 eeg_tensor = torch.FloatTensor(eeg)\n",
    "#                 label_tensor = torch.LongTensor([label])\n",
    "                \n",
    "#                 self.samples.append((signature_tensor, eeg_tensor, label_tensor))\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.samples[idx]\n",
    "\n",
    "# def collate_fn(batch):\n",
    "    \n",
    "#     signatures, eegs, labels = zip(*batch)\n",
    "    \n",
    "#     # Pad sequences to the same length\n",
    "#     signatures_padded = torch.nn.utils.rnn.pad_sequence(signatures, batch_first=True)\n",
    "#     eegs_padded = torch.nn.utils.rnn.pad_sequence(eegs, batch_first=True)\n",
    "    \n",
    "#     labels = torch.cat(labels)\n",
    "    \n",
    "#     return signatures_padded, eegs_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae257955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SignatureEEGTransformer(nn.Module):\n",
    "#     def __init__(self, signature_dim=7, eeg_channels=10, d_model=128, nhead=4, num_layers=3, num_classes=2):\n",
    "#         super(SignatureEEGTransformer, self).__init__()\n",
    "        \n",
    "#         # Signature embedding\n",
    "#         self.signature_embedding = nn.Linear(signature_dim, d_model)\n",
    "        \n",
    "#         # EEG embedding\n",
    "#         self.eeg_embedding = nn.Linear(eeg_channels, d_model)\n",
    "        \n",
    "#         # Positional encoding\n",
    "#         self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "#         # Transformer encoder\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "#         # Classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(d_model * 2, d_model),  # *2 because we concatenate signature and EEG features\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(d_model, num_classes)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, signature, eeg):\n",
    "#         # Signature processing\n",
    "#         signature_embedded = self.signature_embedding(signature)\n",
    "#         signature_embedded = self.positional_encoding(signature_embedded)\n",
    "        \n",
    "#         # EEG processing\n",
    "#         eeg_embedded = self.eeg_embedding(eeg)\n",
    "#         eeg_embedded = self.positional_encoding(eeg_embedded)\n",
    "        \n",
    "#         # Transformer expects (seq_len, batch, features)\n",
    "#         signature_embedded = signature_embedded.permute(1, 0, 2)\n",
    "#         eeg_embedded = eeg_embedded.permute(1, 0, 2)\n",
    "        \n",
    "#         # Process through transformer\n",
    "#         signature_features = self.transformer_encoder(signature_embedded)\n",
    "#         eeg_features = self.transformer_encoder(eeg_embedded)\n",
    "        \n",
    "#         # Average over time dimension\n",
    "#         signature_features = signature_features.mean(dim=0)\n",
    "#         eeg_features = eeg_features.mean(dim=0)\n",
    "        \n",
    "#         # Concatenate features\n",
    "#         combined_features = torch.cat([signature_features, eeg_features], dim=1)\n",
    "        \n",
    "#         # Classify\n",
    "#         output = self.classifier(combined_features)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(signature_data, eeg_data, labels, epochs=20, batch_size=32):\n",
    "#     # Create dataset\n",
    "#     dataset = SignatureEEGDataset(signature_data, eeg_data, labels)\n",
    "    \n",
    "#     # Split into train and validation\n",
    "#     train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Create dataloaders\n",
    "#     train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "#     val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "#     # Initialize model\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = SignatureEEGTransformer().to(device)\n",
    "    \n",
    "#     # Loss and optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "        \n",
    "#         for signatures, eegs, labels in train_loader:\n",
    "#             signatures, eegs, labels = signatures.to(device), eegs.to(device), labels.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             outputs = model(signatures, eegs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss += loss.item()\n",
    "        \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for signatures, eegs, labels in val_loader:\n",
    "#                 signatures, eegs, labels = signatures.to(device), eegs.to(device), labels.to(device)\n",
    "                \n",
    "#                 outputs = model(signatures, eegs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         print(f'Epoch {epoch+1}/{epochs}')\n",
    "#         print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n",
    "#         print(f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "#         print(f'Val Accuracy: {100*correct/total:.2f}%')\n",
    "#         print('-' * 50)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4996d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(normalized_sign_data_dict, normalized_eeg_data_dict, user_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# def prepare_data_with_masking(signature_data, eeg_data, labels_dict):\n",
    "#     \"\"\"\n",
    "#     Prepares data with dynamic padding and preserves original lengths for masking.\n",
    "#     Returns:\n",
    "#         - Padded sequences\n",
    "#         - Sequence length arrays (for masking)\n",
    "#         - Labels\n",
    "#     \"\"\"\n",
    "#     # Initialize lists\n",
    "#     X_signature, X_eeg = [], []\n",
    "#     len_signature, len_eeg = [], []\n",
    "#     y = []\n",
    "    \n",
    "#     for user_id in labels_dict.keys():\n",
    "#         for i in range(len(labels_dict[user_id])):\n",
    "#             # Signature data\n",
    "#             sig = signature_data[user_id][i]\n",
    "#             X_signature.append(sig)\n",
    "#             len_signature.append(len(sig))\n",
    "            \n",
    "#             # EEG data\n",
    "#             eeg = eeg_data[user_id][i]\n",
    "#             X_eeg.append(eeg)\n",
    "#             len_eeg.append(len(eeg))\n",
    "            \n",
    "#             # Label\n",
    "#             y.append(labels_dict[user_id][i])\n",
    "    \n",
    "#     # Pad sequences to max length in dataset\n",
    "#     max_len_sig = max(len_signature)\n",
    "#     max_len_eeg = max(len_eeg)\n",
    "    \n",
    "#     X_signature_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#         X_signature, maxlen=max_len_sig, dtype='float32', padding='post'\n",
    "#     )\n",
    "    \n",
    "#     X_eeg_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#         X_eeg, maxlen=max_len_eeg, dtype='float32', padding='post'\n",
    "#     )\n",
    "    \n",
    "#     return (\n",
    "#         X_signature_pad, np.array(len_signature),\n",
    "#         X_eeg_pad, np.array(len_eeg),\n",
    "#         np.array(y)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0cc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_masked_model(signature_shape, eeg_shape):\n",
    "#     \"\"\"Creates a model with masking layers to ignore padding\"\"\"\n",
    "#     # Signature branch\n",
    "#     signature_input = layers.Input(shape=signature_shape, name='signature_input')\n",
    "#     signature_length = layers.Input(shape=(1,), name='signature_length', dtype='int32')\n",
    "    \n",
    "#     sig = layers.Masking(mask_value=0.0)(signature_input)\n",
    "#     sig = layers.Conv1D(32, 5, activation='relu', padding='same')(sig)\n",
    "#     sig = layers.MaxPooling1D(2)(sig)\n",
    "#     sig = layers.Conv1D(64, 5, activation='relu', padding='same')(sig)\n",
    "#     sig = layers.MaxPooling1D(2)(sig)\n",
    "#     sig = layers.GlobalAveragePooling1D()(sig)\n",
    "    \n",
    "#     # EEG branch\n",
    "#     eeg_input = layers.Input(shape=eeg_shape, name='eeg_input')\n",
    "#     eeg_length = layers.Input(shape=(1,), name='eeg_length', dtype='int32')\n",
    "    \n",
    "#     eeg = layers.Masking(mask_value=0.0)(eeg_input)\n",
    "#     eeg = layers.Conv1D(32, 5, activation='relu', padding='same')(eeg)\n",
    "#     eeg = layers.MaxPooling1D(2)(eeg)\n",
    "#     eeg = layers.Conv1D(64, 5, activation='relu', padding='same')(eeg)\n",
    "#     eeg = layers.MaxPooling1D(2)(eeg)\n",
    "#     eeg = layers.GlobalAveragePooling1D()(eeg)\n",
    "    \n",
    "#     # Combine branches\n",
    "#     combined = layers.concatenate([sig, eeg])\n",
    "#     combined = layers.Dense(128, activation='relu')(combined)\n",
    "#     outputs = layers.Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "#     # Model with length inputs\n",
    "#     model = models.Model(\n",
    "#         inputs=[signature_input, signature_length, eeg_input, eeg_length],\n",
    "#         outputs=outputs\n",
    "#     )\n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#                  loss='binary_crossentropy',\n",
    "#                  metrics=['accuracy'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X_sig, len_sig, X_eeg, len_eeg, y, batch_size=32):\n",
    "        self.X_sig = X_sig\n",
    "        self.len_sig = len_sig\n",
    "        self.X_eeg = X_eeg\n",
    "        self.len_eeg = len_eeg\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.y) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_sig = self.X_sig[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_len_sig = self.len_sig[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_eeg = self.X_eeg[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_len_eeg = self.len_eeg[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_y = self.y[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        \n",
    "        return [batch_sig, batch_len_sig, batch_eeg, batch_len_eeg], batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ae07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_authentication_model(signature_data, eeg_data, labels_dict):\n",
    "#     # Prepare data\n",
    "#     X_signature, X_eeg, y = prepare_data(signature_data, eeg_data, labels_dict)\n",
    "    \n",
    "#     # Split into train and test sets\n",
    "#     (X_signature_train, X_signature_test, \n",
    "#      X_eeg_train, X_eeg_test, \n",
    "#      y_train, y_test) = train_test_split(X_signature, X_eeg, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Create model\n",
    "#     signature_shape = X_signature_train.shape[1:]\n",
    "#     eeg_shape = X_eeg_train.shape[1:]\n",
    "#     model = create_dual_input_model(signature_shape, eeg_shape)\n",
    "    \n",
    "#     # Train model\n",
    "#     history = model.fit(\n",
    "#         [X_signature_train, X_eeg_train],\n",
    "#         y_train,\n",
    "#         epochs=50,\n",
    "#         batch_size=32,\n",
    "#         validation_data=([X_signature_test, X_eeg_test], y_test),\n",
    "#         callbacks=[\n",
    "#             tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "#             tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare data\n",
    "X_sig, len_sig, X_eeg, len_eeg, norm_y = prepare_data_with_masking(signature_data, eeg_data, labels_dict)\n",
    "\n",
    "# 2. Split data\n",
    "(X_sig_train, X_sig_test, \n",
    " len_sig_train, len_sig_test,\n",
    " X_eeg_train, X_eeg_test,\n",
    " len_eeg_train, len_eeg_test,\n",
    " y_train, y_test) = train_test_split(X_sig, len_sig, X_eeg, len_eeg, norm_y, test_size=0.2)\n",
    "\n",
    "# 3. Create model\n",
    "model = create_masked_model(X_sig_train.shape[1:], X_eeg_train.shape[1:])\n",
    "\n",
    "# 4. Create generators\n",
    "train_gen = DataGenerator(X_sig_train, len_sig_train, X_eeg_train, len_eeg_train, y_train)\n",
    "val_gen = DataGenerator(X_sig_test, len_sig_test, X_eeg_test, len_eeg_test, y_test)\n",
    "\n",
    "# 5. Train\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e718c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_signature_test, X_eeg_test, y_test):\n",
    "    results = model.evaluate([X_signature_test, X_eeg_test], y_test)\n",
    "    print(f\"Test Loss: {results[0]:.4f}\")\n",
    "    print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "    print(f\"Test AUC: {results[2]:.4f}\")\n",
    "    \n",
    "    # You can add more evaluation metrics as needed\n",
    "    y_pred = model.predict([X_signature_test, X_eeg_test])\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
