{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6e58c6",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "853f3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import constants\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import scipy.io as sp\n",
    "import pprint as pp\n",
    "from scipy.signal import welch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2fe3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=300, suppress=True)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4bc15",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2b9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some calculations\n",
    "# for the dataset, 10 seconds = 1280 frames. 1 second = 128 frames. We can sue this to find the number of seconds that were taken to record the signature.\n",
    "\n",
    "recording_samp_rate = 128 # per second\n",
    "per_phase_frames = 1280 # seconds\n",
    "max_seq_len_for_data = 3000 # frames\n",
    "# desampling_factor = 1 # reducing sequences by this size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ee960",
   "metadata": {},
   "source": [
    "# Fetching raw data (Sign + EEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358fbdd",
   "metadata": {},
   "source": [
    "## Processing raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6301463",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "dataset_path = os.getenv('DATASET_PATH')\n",
    "\n",
    "def get_dataset_files_and_user_ids(data_category, data_type = constants.TRAIN):\n",
    "    user_ids = []\n",
    "    labels = []\n",
    "    files_mat = []\n",
    "    \n",
    "    # Get training and testing data\n",
    "    data_split = pd.read_csv(os.path.join(dataset_path, \"Identification_split.csv\"))\n",
    "    files_for_task = list(data_split[data_split.set == data_type].filename)\n",
    "    data_categories = [constants.GENUINE, constants.FORGED] if data_category == constants.ALL else [data_category]\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        if os.path.basename(root) in data_categories:\n",
    "            for file in files:\n",
    "                if file.endswith('.mat') and file in files_for_task:\n",
    "                    files_mat.append(os.path.join(root, file))\n",
    "                    labels.append(os.path.basename(root))\n",
    "        if os.path.basename(root) != constants.GENUINE and os.path.basename(root) != constants.FORGED and os.path.basename(root) != 'SignEEGv1.0':\n",
    "            user_ids.append(os.path.basename(root))\n",
    "        \n",
    "    return files_mat, user_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small note: np.delete(axis = 1) will delete a column, axis = 0 will delete a row. be careful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69bfe1d",
   "metadata": {},
   "source": [
    "## Getting list of sign data, eeg data and label for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c4f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_eeg_raw_data(mat_files, labels, desampling_factor = 1):\n",
    "    raw_data_list = []\n",
    "    for mat_file, label in zip(mat_files, labels):\n",
    "        mat_content = sp.loadmat(mat_file)\n",
    "        user_id = str(mat_content['subject']['SubjectID'][0][0][0])\n",
    "        sig_data = mat_content['subject']['SignWacom'][0][0]\n",
    "        eeg_ica_data = mat_content['subject']['ICA_EEG'][0][0].T\n",
    "        sig_data = torch.from_numpy(np.delete(sig_data, 0, axis=1)).to(dtype=torch.float32)\n",
    "        \n",
    "        # getting part of eeg data during which signature was recorded (ROI)\n",
    "        roi_frames_start = -(eeg_ica_data.shape[0] % per_phase_frames) if per_phase_frames > 0 else 0\n",
    "        eeg_ica_data = torch.from_numpy(eeg_ica_data[roi_frames_start:]).to(dtype=torch.float32)\n",
    "\n",
    "        # desampling the data\n",
    "        if desampling_factor > 1:\n",
    "            sig_data = sig_data[::desampling_factor, :]\n",
    "            eeg_ica_data = eeg_ica_data[::desampling_factor, :]\n",
    "\n",
    "        if sig_data.shape[0] > max_seq_len_for_data:\n",
    "            # print(\"Caught you!!!\")\n",
    "            # print(\"User ID: \", user_id)\n",
    "            # print(\"File: \", mat_file)  \n",
    "            continue # Skip these files because it's too long, outlier\n",
    "        raw_data_list.append({\n",
    "            'sign_data': sig_data,\n",
    "            'eeg_data': eeg_ica_data,\n",
    "            'user_id': user_id,\n",
    "            'label': 0 if label == constants.GENUINE else 1,\n",
    "            'file': mat_file\n",
    "        })\n",
    "\n",
    "    return raw_data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a9078",
   "metadata": {},
   "source": [
    "# Augmenting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac35e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sign_data(sign_data, noise_std=0.01, scale_range=(0.95, 1.05), rotation_deg=5):\n",
    "    augmented = sign_data.clone()\n",
    "    augmented[:, 2:] += torch.randn_like(augmented[:, 2:]) * noise_std\n",
    "    scale = random.uniform(*scale_range)\n",
    "    augmented[:, 2] *= scale\n",
    "    augmented[:, 3] *= scale\n",
    "\n",
    "    # Random rotation (x, y)\n",
    "    theta = math.radians(random.uniform(-rotation_deg, rotation_deg))\n",
    "    x = augmented[:, 2].clone()\n",
    "    y = augmented[:, 3].clone()\n",
    "    augmented[:, 2] = x * math.cos(theta) - y * math.sin(theta)\n",
    "    augmented[:, 3] = x * math.sin(theta) + y * math.cos(theta)\n",
    "\n",
    "    return augmented\n",
    "\n",
    "def augment_eeg_data(eeg_data, noise_std=0.01, scale_range=(0.95, 1.05), time_shift_max=10):\n",
    "    augmented = eeg_data.clone()\n",
    "    augmented += torch.randn_like(augmented) * noise_std\n",
    "    scale = random.uniform(*scale_range)\n",
    "    augmented *= scale\n",
    "    shift = random.randint(-time_shift_max, time_shift_max)\n",
    "    if shift > 0:\n",
    "        augmented = torch.cat([augmented[shift:], torch.zeros_like(augmented[:shift])], dim=0)\n",
    "    elif shift < 0:\n",
    "        augmented = torch.cat([torch.zeros_like(augmented[shift:]), augmented[:shift]], dim=0)\n",
    "\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d478c1",
   "metadata": {},
   "source": [
    "# Sign Data Classification Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb8429",
   "metadata": {},
   "source": [
    "## Sign data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a2b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sign_data_dict(sign_data):\n",
    "\n",
    "    mean = torch.mean(sign_data[:, 2:], dim=0)\n",
    "    std = torch.std(sign_data[:, 2:], dim=0)\n",
    "    std = torch.where(std == 0, torch.tensor(1.0, dtype=torch.float32), std)\n",
    "    normalized = (sign_data[:, 2:] - mean) / std\n",
    "    normalized = torch.cat([sign_data[:, 0:2], normalized], dim=1).to(dtype=torch.float32)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b972dd",
   "metadata": {},
   "source": [
    "## Sign Data Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eadc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sign_data_features(sign_data):\n",
    "    normalized_sign_data = normalize_sign_data_dict(sign_data)\n",
    "    x = sign_data[:, 2]\n",
    "    y = sign_data[:, 3]\n",
    "\n",
    "    normalized_sign_data = torch.tensor(normalized_sign_data, dtype=torch.float32)\n",
    "    norm_x = normalized_sign_data[:, 2]\n",
    "    norm_y = normalized_sign_data[:, 3]\n",
    "    vx = torch.gradient(norm_x)[0]\n",
    "    vy = torch.gradient(norm_y)[0]\n",
    "    velocity = torch.sqrt(vx**2 + vy**2)\n",
    "    ax = torch.gradient(vx)[0]\n",
    "    ay = torch.gradient(vy)[0]\n",
    "    acceleration = torch.sqrt(ax**2 + ay**2)\n",
    "    \n",
    "    avg_vx = torch.mean(vx)\n",
    "    avg_vy = torch.mean(vy)\n",
    "    avg_ax = torch.mean(ax)\n",
    "    avg_ay = torch.mean(ay)\n",
    "    \n",
    "    # log curvature radius\n",
    "    dt = 1\n",
    "    dx = torch.gradient(norm_x, spacing=(dt,))[0]\n",
    "    dy = torch.gradient(norm_y, spacing=(dt,))[0]\n",
    "    v_t = torch.sqrt(dx ** 2 + dy ** 2)\n",
    "    v_t = torch.where(v_t == 0, torch.tensor(1e-10, dtype=v_t.dtype), v_t)\n",
    "    theta = torch.atan2(dy, dx)\n",
    "    dtheta = torch.gradient(theta, spacing=(dt,))[0]\n",
    "    dtheta = torch.where(dtheta == 0, torch.tensor(1e-10, dtype=dtheta.dtype), dtheta)\n",
    "    log_curv_radius = torch.log(torch.abs(v_t / dtheta) + 1e-10)\n",
    "    # print(\"Log Curve Radius shape: \", log_curv_radius.shape)\n",
    "    # getting static features\n",
    "    pendown_frames = normalized_sign_data[:, 1] == 1\n",
    "    num_strokes = torch.unique(normalized_sign_data[pendown_frames][:, 0]).shape[0]\n",
    "    x_down = normalized_sign_data[pendown_frames][:, 2]\n",
    "    y_down = normalized_sign_data[pendown_frames][:, 3]\n",
    "    sign_centroid = torch.tensor([torch.mean(x_down), torch.mean(y_down)], dtype=torch.float32)\n",
    "    if y_down.shape[0] > 0:\n",
    "        sign_height = torch.max(y_down) - torch.min(y_down)\n",
    "    else:\n",
    "        sign_height = 0\n",
    "    if x_down.shape[0] > 0:\n",
    "        sign_width = torch.max(x_down) - torch.min(x_down)\n",
    "    else:\n",
    "        sign_width = 0\n",
    "    height_width_ratio = sign_height / sign_width if sign_width != 0 else torch.tensor(0.0, dtype=torch.float32)\n",
    "    \n",
    "    # new time dependent feature - jerk\n",
    "    jerk = torch.sqrt(torch.gradient(vx)[0]**2 + torch.gradient(vy)[0]**2)\n",
    "\n",
    "    pressure = sign_data[pendown_frames][:, 4]\n",
    "    azimuth = sign_data[pendown_frames][:, 5]\n",
    "    altitude = sign_data[pendown_frames][:, 6]\n",
    "    avg_pressure = torch.mean(pressure)\n",
    "    avg_azimuth = torch.mean(azimuth)\n",
    "    avg_altitude = torch.mean(altitude)\n",
    "    max_pressure = torch.max(pressure) if pressure.numel() > 0 else torch.tensor(0.0, dtype=torch.float32)\n",
    "    sign_duration = sign_data.shape[0] / recording_samp_rate\n",
    "    cls_token = torch.tensor([\n",
    "        num_strokes, sign_height, sign_width, height_width_ratio, sign_centroid[0], sign_centroid[1], avg_pressure, avg_azimuth, avg_altitude, avg_vx, avg_vy, avg_ax, avg_ay, max_pressure, sign_duration], dtype=torch.float32)\n",
    "    sign_data_aug = torch.cat([normalized_sign_data, velocity.unsqueeze(1), acceleration.unsqueeze(1), log_curv_radius.unsqueeze(1), jerk.unsqueeze(1)], dim=1)\n",
    "\n",
    "    return sign_data_aug, cls_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75128f7a",
   "metadata": {},
   "source": [
    "## Prepare sign dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb40603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_attention_tokens_and_padding(data, max_len):\n",
    "    # print(\"EEG Data shape: \", data.shape)\n",
    "    if data.shape[0] == 0:\n",
    "        feat_dim = data.shape[1] if data.ndim == 2 else 1\n",
    "        return torch.zeros((max_len, feat_dim), dtype=torch.float32), torch.zeros(max_len, dtype=torch.float32)\n",
    "    seq_len, feat_dim = data.shape\n",
    "    pad_width = (0, max_len - seq_len)\n",
    "    padded_data = torch.nn.functional.pad(data, (0, 0, 0, pad_width[1]), mode='constant', value=0)\n",
    "    attention_mask = torch.zeros(max_len + 1, dtype=torch.float32)\n",
    "    attention_mask[:seq_len + 1] = 1  # +1 for cls_token\n",
    "    return padded_data, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For single data\n",
    "\n",
    "# sign_dict_sample, eeg_dict_sample, label = get_sig_eeg_data_dicts([files_mat_appended[0]], [1])\n",
    "# # print(sign_dict_sample['000000000200894'][0].shape)\n",
    "# normalized_sign_data_sample = normalize_sign_data_dict(sign_dict_sample)\n",
    "# sign_with_features = get_sign_data_features(normalized_sign_data_sample)\n",
    "# # print()\n",
    "# final_sign_data = sign_attach_attention_tokens_and_labels(sign_with_features, label)\n",
    "# final_sign_dataset = prepare_sign_dataset_with_all_parts(final_sign_data)\n",
    "# # print(\"Max len: \", get_sign_max_seq_len(sign_with_features))\n",
    "# # print(final_sign_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33ebbb",
   "metadata": {},
   "source": [
    "## Sign Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedde36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes will be 2 (0 = unauthenticated, 1 = authenticated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d20fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' current data format:\n",
    "\n",
    "Just for reference, so that I don't forget later\n",
    "\n",
    "original data: \n",
    "\n",
    "{\n",
    "    user_id: string,\n",
    "    data: [ [ [], [], [] ... ], [ [], [], [] ... ] ...  ], size: (30 * 1200 * 7) = (num_samples_per_user * time_series_len * num_features)\n",
    "}\n",
    "\n",
    "extracted data:\n",
    "\n",
    "{\n",
    "    cls_tokens: tensor([ , , , , ... ], [ , , , ,  ....], [ , , , , ...] ... ), size: (total_num_samples * num_features),\n",
    "    data: tensor([ [], [], [] ... ], [ [], [], [] ... ] ...), size: (total_num_samples * time_series_len * num_features),\n",
    "    attention_masks: tensor([1, 1, 1, ...], [1, 1,1, ...] ... ), size: (total_num_samples * time_series_len),\n",
    "    labels: tensor([1, 0, 1, 0 ...]), size: (total_num_samples, )\n",
    "}\n",
    "'''\n",
    "\n",
    "class SignatureDataset(Dataset):\n",
    "    def __init__(self, input_data, num_classes):\n",
    "        self.seq_len = input_data['sign_data'][0].shape[0]\n",
    "        self.ts_dim = input_data['sign_data'][0].shape[1]\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.x_ts = input_data['sign_data']\n",
    "        self.cls_token = input_data['sign_cls_tokens']\n",
    "        self.labels = input_data['labels']\n",
    "        self.attention_mask = input_data['sign_attention_masks']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x_ts': self.x_ts[idx],\n",
    "            'cls_token': self.cls_token[idx],\n",
    "            'labels': self.labels[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451ebf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len + 1, d_model)\n",
    "        position = torch.arange(0, max_len + 1, dtype = torch.float).unsqueeze(1)\n",
    "        divterm = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Credits to hkproj@github for this as https://github.com/hkproj/pytorch-transformer/blob/main/model.py\n",
    "        pe[:, 0::2] = torch.sin(position * divterm)\n",
    "        pe[:, 1::2] = torch.cos(position * divterm)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ab58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, cls_dim, d_model, num_classes, num_heads, num_layers, max_seq_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.cls_proj = nn.Linear(cls_dim, d_model)\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dropout = dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        # uncomment for single modality\n",
    "        self.classifier = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, x_ts, cls_token, attn_mask = None):\n",
    "        x_ts = torch.nan_to_num(x_ts, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        cls_token = torch.nan_to_num(cls_token, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        batch_size, t, feat_dim = x_ts.shape\n",
    "        x_proj = self.input_projection(x_ts)\n",
    "        cls_proj = self.cls_proj(cls_token).unsqueeze(1)\n",
    "        # print(\"x_proj size: \", x_proj.shape)\n",
    "        # print(\"cls_proj size: \", cls_proj.shape)\n",
    "        x = torch.cat([cls_proj, x_proj], dim=1)\n",
    "        # print(\"x_proj and cls_proj concatenated size: \", x.shape)\n",
    "        x = x + self.positional_encoding(x)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask == 0 # True = ignore the value, False = include it!!!!!!!!!!\n",
    "            # cls_mask = torch.zeros((batch_size, 1), dtype=torch.bool, device=attn_mask.device)\n",
    "            full_mask = torch.cat([attn_mask], dim=1)  # [batch_size, t+1]\n",
    "        else:\n",
    "            full_mask = None\n",
    "        # print(\"Mask shape: \", full_mask.shape)\n",
    "        x = self.transformer(x, src_key_padding_mask=full_mask)\n",
    "        cls_output = x[:, 0, :]\n",
    "        # uncomment for single modality transformer\n",
    "        # logits = self.classifier(cls_output)\n",
    "        # return logits\n",
    "\n",
    "        # uncomment for multimodal transformer\n",
    "        return cls_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbdd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def sign_training_loop(model, dataloader, optimizer, loss_fn, device, num_epochs=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    model_path = os.getenv(\"MODEL_PATH\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        total_samples = 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "            x_ts = batch['x_ts'].to(device)\n",
    "            cls_token = batch['cls_token'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_ts, cls_token, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print(\"NaN loss detected!\")\n",
    "                print(\"Labels:\", labels)\n",
    "                print(\"Logits:\", logits)\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x_ts.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_samples += x_ts.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        print(f\"Loss: {avg_loss:.4f} | Acc: {acc:.4f} | Prec: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, f\"model{datetime.now().strftime('%m%d%Y-%H%M%S')}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b8f2a",
   "metadata": {},
   "source": [
    "## Sign Data Feed to Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'sign_data': sign_data,\n",
    "    'eeg_data': eeg_data,\n",
    "    'sign_attention_masks': sign_attention_masks,\n",
    "    'eeg_attention_masks': eeg_attention_masks,\n",
    "    'sign_cls_tokens': sign_cls_tokens,\n",
    "    'eeg_cls_tokens': eeg_cls_tokens,\n",
    "    'labels': labels,\n",
    "}\n",
    "ts_dim = input_data['sign_data'][0].size(1)\n",
    "cls_dim = input_data['sign_cls_tokens'][0].size(0)\n",
    "d_model = 64\n",
    "num_classes = 2\n",
    "seq_len = max([data.shape[0] for data in input_data['sign_data']])\n",
    "batch_size = 8\n",
    "\n",
    "dataset = SignatureDataset(input_data, num_classes)\n",
    "# dataset.__getitem__(0)['x_ts'].shape\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "sign_model = SignatureTransformer(input_dim=ts_dim, cls_dim=cls_dim, d_model=d_model, num_classes=num_classes, num_heads=4, num_layers=2, max_seq_len=seq_len)\n",
    "optimizer = optim.Adam(sign_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "sign_training_loop(sign_model, dataloader, optimizer, loss_fn, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a51e3b",
   "metadata": {},
   "source": [
    "## Sign - Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(len(final_signature_data['002108410100044']['cls_tokens']))\n",
    "# # print(len(final_signature_data['002108410100044']['data']))\n",
    "# # print(len(final_signature_data['002108410100044']['attention_masks']))\n",
    "\n",
    "# # print(final_signature_data['002108410100044']['cls_tokens'][0].shape)\n",
    "# # print(final_signature_data['002108410100044']['data'][0].shape)\n",
    "# # print(final_signature_data['002108410100044']['attention_masks'][0].shape)\n",
    "\n",
    "\n",
    "# print(len(final_sign_dataset_for_all_users['cls_tokens']))\n",
    "# print(len(final_sign_dataset_for_all_users['data']))\n",
    "# print(len(final_sign_dataset_for_all_users['attention_masks']))\n",
    "# print(len(final_sign_dataset_for_all_users['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec3dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(len(final_sign_dataset_for_all_users['cls_tokens']))\n",
    "# print(len(final_sign_dataset_for_all_users['data']))\n",
    "# print(len(final_sign_dataset_for_all_users['attention_masks']))\n",
    "\n",
    "# print(final_sign_dataset_for_all_users['cls_tokens'][0].shape)\n",
    "# print(final_sign_dataset_for_all_users['data'][0].shape)\n",
    "# print(final_sign_dataset_for_all_users['attention_masks'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93df9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(normalized_sign_data_dict['000000000200894'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_sign_data_dict = normalize_sign_data_dict(sign_data_dict)\n",
    "# # print(sign_data_dict['000000000200894'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4fc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(sign_times_for_users))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab25fe",
   "metadata": {},
   "source": [
    "# EEG Data Classification Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fe1a5",
   "metadata": {},
   "source": [
    "## EEG Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e12b2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_eeg_data_dict(eeg_data_dict):\n",
    "    normalized_eeg_data_dict = {}\n",
    "    for user_id, eeg_list in eeg_data_dict.items():\n",
    "        normalized_eeg_data_dict[user_id] = []\n",
    "        for eeg_data in eeg_list:\n",
    "            mean = eeg_data.mean(dim=0, keepdim=True)\n",
    "            std = eeg_data.std(dim=0, keepdim=True)\n",
    "            std = torch.where(std == 0, torch.tensor(1.0, dtype=std.dtype, device=std.device), std)\n",
    "            normalized = (eeg_data - mean) / std\n",
    "            normalized_eeg_data_dict[user_id].append(normalized)\n",
    "    return normalized_eeg_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c86c80",
   "metadata": {},
   "source": [
    "## EEG Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613e78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fft_features(eeg_data, fs=128, epoch_length_sec=1): # taking 1s instead of 30s because the signal duration is short\n",
    "    n_samples, n_channels = eeg_data.shape\n",
    "    epoch_len = int(epoch_length_sec * fs)\n",
    "    n_epochs = n_samples // epoch_len\n",
    "    features = []\n",
    "    for i in range(n_epochs):\n",
    "        epoch = eeg_data[i*epoch_len:(i+1)*epoch_len, :]\n",
    "        epoch_features = []\n",
    "        for ch in range(n_channels):\n",
    "            fft_vals = np.fft.rfft(epoch[:, ch])\n",
    "            fft_power = np.abs(fft_vals)\n",
    "            epoch_features.extend(np.abs(fft_vals).flatten())\n",
    "        features.append(epoch_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c6685fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nth_difference_mean_for_signal(input_signal, n):\n",
    "    input_signal = torch.as_tensor(input_signal)\n",
    "    diff = torch.abs(input_signal[n:] - input_signal[:-n])\n",
    "    res = torch.sum(diff) / (input_signal.shape[0] - n)\n",
    "    return res\n",
    "\n",
    "def normalize_for_eeg_related_data(data):\n",
    "    data = torch.as_tensor(data, dtype=torch.float32)\n",
    "    mean = torch.mean(data, dim=0)\n",
    "    std = torch.std(data, dim=0)\n",
    "    std = torch.where(std == 0, torch.tensor(1.0, dtype=std.dtype, device=std.device), std)\n",
    "    norm = (data - mean) / std\n",
    "    return norm\n",
    "\n",
    "def get_eeg_data_features(eeg_data, fs=recording_samp_rate):\n",
    "\n",
    "    signal_mean = torch.mean(eeg_data)\n",
    "    signal_std = torch.std(eeg_data)\n",
    "\n",
    "    first_difference_sample_mean_absolute_difference_raw_signal = get_nth_difference_mean_for_signal(eeg_data, 1)\n",
    "    second_difference_sample_mean_absolute_difference_raw_signal = get_nth_difference_mean_for_signal(eeg_data, 2)\n",
    "\n",
    "    normalized_signal = normalize_for_eeg_related_data(eeg_data)\n",
    "    first_difference_sample_mean_absolute_difference_normalized_signal = get_nth_difference_mean_for_signal(normalized_signal, 1)\n",
    "    second_difference_sample_mean_absolute_difference_normalized_signal = get_nth_difference_mean_for_signal(normalized_signal, 2)\n",
    "    fw_powers = []\n",
    "    eeg_data = torch.as_tensor(eeg_data, dtype=torch.float32)\n",
    "    for ch in range(normalized_signal.shape[1]):\n",
    "        # Welch returns numpy arrays, so convert to torch\n",
    "        f, Pxx = welch(normalized_signal[:, ch].cpu().numpy(), fs=fs)\n",
    "        f = torch.from_numpy(f).to(normalized_signal.device)\n",
    "        Pxx = torch.from_numpy(Pxx).to(normalized_signal.device)\n",
    "        fw_power = torch.sum(f * Pxx) / torch.sum(Pxx) if torch.sum(Pxx) > 0 else torch.tensor(0.0, device=normalized_signal.device)\n",
    "        fw_powers.append(fw_power)\n",
    "    fw_power_arr = torch.stack(fw_powers).unsqueeze(0)\n",
    "    # cls_token = torch.cat([signal_mean, signal_std, first_difference_sample_mean_absolute_difference_raw_signal, second_difference_sample_mean_absolute_difference_raw_signal, first_difference_sample_mean_absolute_difference_normalized_signal, second_difference_sample_mean_absolute_difference_normalized_signal])\n",
    "    # cls_token = torch.stack(fw_power_arr)\n",
    "    fft_features = extract_fft_features(normalized_signal.cpu().numpy(), fs=fs) # because we are using np.fft.rfft\n",
    "    features = [signal_mean, signal_std, first_difference_sample_mean_absolute_difference_raw_signal, second_difference_sample_mean_absolute_difference_raw_signal, first_difference_sample_mean_absolute_difference_normalized_signal, second_difference_sample_mean_absolute_difference_normalized_signal]\n",
    "    # for epoch_feat in fft_features:\n",
    "    #     features.extend(epoch_feat)\n",
    "    features.extend(fw_power_arr.squeeze(0).tolist())\n",
    "    cls_token = torch.tensor(features, dtype=torch.float32)\n",
    "    # uncomment if fft_features as eeg_data fails\n",
    "    # return normalized_signal, cls_token\n",
    "    fft_features = torch.tensor(fft_features, dtype=torch.float32)\n",
    "    fft_features = torch.nan_to_num(fft_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return normalized_signal, fft_features, cls_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36432dfd",
   "metadata": {},
   "source": [
    "## EEG Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All EEG Data\n",
    "\n",
    "# files_mat_genuine, user_ids_genuine, genuine_labels = get_dataset_files_and_user_ids(data_category=constants.GENUINE)\n",
    "# files_mat_forged, user_ids_forged, forged_labels = get_dataset_files_and_user_ids(data_category=constants.FORGED)\n",
    "\n",
    "# files_mat_genuine.extend(files_mat_forged)\n",
    "# files_mat_appended = files_mat_genuine\n",
    "# genuine_labels.extend(forged_labels)\n",
    "# labels_appended = genuine_labels\n",
    "\n",
    "# # shuffling to prevent overfitting\n",
    "# files_all = np.array(files_mat_appended)\n",
    "# labels_all = np.array(labels_appended)\n",
    "\n",
    "# indices = np.arange(len(files_all))\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# files_mat_appended = files_all[indices]\n",
    "# labels_appended = labels_all[indices]\n",
    "\n",
    "# sign_data_dict, eeg_data_dict, labels = get_sig_eeg_raw_data(files_mat_appended, labels_appended)\n",
    "# normalized_eeg_data_dict = normalize_eeg_data_dict(eeg_data_dict)\n",
    "# eeg_data_with_features = get_eeg_data_features(normalized_eeg_data_dict)\n",
    "# eeg_final_data = eeg_attach_attention_tokens_and_labels(eeg_data_with_features, labels)\n",
    "# eeg_final_dataset = prepare_eeg_dataset_with_all_parts(eeg_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = eeg_final_dataset\n",
    "# ts_dim = input_data['data'][0].size(1)\n",
    "# cls_dim = input_data['cls_tokens'][0].size(0)\n",
    "# d_model = 64\n",
    "# num_classes = 2\n",
    "# seq_len = get_eeg_max_seq_len(eeg_data_with_features)\n",
    "# batch_size = 8\n",
    "\n",
    "# dataset = SignatureDataset(input_data, num_classes)\n",
    "# # dataset.__getitem__(0)['x_ts'].shape\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# eeg_model = SignatureTransformer(input_dim=ts_dim, cls_dim=cls_dim, d_model=d_model, num_classes=num_classes, num_heads=4, num_layers=4, max_seq_len=seq_len)\n",
    "# optimizer = optim.Adam(eeg_model.parameters(), lr=1e-4)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# sign_training_loop(eeg_model, dataloader, optimizer, loss_fn, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f624e0",
   "metadata": {},
   "source": [
    "# Sign + EEG Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15581d",
   "metadata": {},
   "source": [
    "## Getting the EEG and Sign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314126b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_mat_genuine, user_ids_genuine, genuine_labels = get_dataset_files_and_user_ids(data_category=constants.GENUINE)\n",
    "files_mat_forged, user_ids_forged, forged_labels = get_dataset_files_and_user_ids(data_category=constants.FORGED)\n",
    "\n",
    "files_mat_genuine.extend(files_mat_forged)\n",
    "files_mat_appended = files_mat_genuine\n",
    "genuine_labels.extend(forged_labels)\n",
    "labels_appended = genuine_labels\n",
    "\n",
    "# shuffling to prevent overfitting\n",
    "files_all = np.array(files_mat_appended)\n",
    "labels_all = np.array(labels_appended)\n",
    "\n",
    "indices = np.arange(len(files_all))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "files_mat_appended = files_all[indices]\n",
    "labels_appended = labels_all[indices]\n",
    "\n",
    "raw_data = get_sig_eeg_raw_data(files_mat_appended, labels_appended)\n",
    "\n",
    "# for debugging only\n",
    "print(\"EEG Data seq len: \", [data['eeg_data'].shape[0] for data in raw_data])\n",
    "\n",
    "augmented_raw_data = []\n",
    "num_augments = 3\n",
    "\n",
    "for sample in raw_data:\n",
    "    augmented_raw_data.append(sample)\n",
    "    for _ in range(num_augments):\n",
    "        aug_sample = sample.copy()\n",
    "        aug_sample['sign_data'] = augment_sign_data(sample['sign_data'])\n",
    "        aug_sample['eeg_data'] = augment_eeg_data(sample['eeg_data'])\n",
    "        augmented_raw_data.append(aug_sample)\n",
    "\n",
    "raw_data = augmented_raw_data\n",
    "\n",
    "for i in range(len(raw_data)):\n",
    "    sign_data_with_features, sign_cls_token = get_sign_data_features(raw_data[i]['sign_data'])\n",
    "    eeg_data_with_features, eeg_cls_token = get_eeg_data_features(raw_data[i]['eeg_data'])\n",
    "    # print(\"EEG Feature data shape: \", eeg_data_with_features.shape)\n",
    "    raw_data[i]['sign_data'] = sign_data_with_features\n",
    "    raw_data[i]['sign_cls_token'] = sign_cls_token\n",
    "    raw_data[i]['eeg_data'] = eeg_data_with_features\n",
    "    raw_data[i]['eeg_cls_token'] = eeg_cls_token\n",
    "\n",
    "# sign_max_seq_len = max([data['sign_data'].shape[0] for data in raw_data])\n",
    "# eeg_max_seq_len = max([data['eeg_data'].shape[0] for data in raw_data])\n",
    "\n",
    "sign_max_seq_len = max_seq_len_for_data\n",
    "eeg_max_seq_len = 10\n",
    "\n",
    "for i in range(len(raw_data)):\n",
    "    sign_data = raw_data[i]['sign_data']\n",
    "    eeg_data = raw_data[i]['eeg_data']\n",
    "    sign_data, sign_attention_mask = attach_attention_tokens_and_padding(sign_data, sign_max_seq_len)\n",
    "    eeg_data, eeg_attention_mask = attach_attention_tokens_and_padding(eeg_data, eeg_max_seq_len)\n",
    "    raw_data[i]['sign_data'] = sign_data\n",
    "    raw_data[i]['eeg_data'] = eeg_data\n",
    "    raw_data[i]['sign_attention_mask'] = sign_attention_mask\n",
    "    raw_data[i]['eeg_attention_mask'] = eeg_attention_mask\n",
    "\n",
    "sign_data = [data['sign_data'] for data in raw_data]\n",
    "eeg_data = [data['eeg_data'] for data in raw_data]\n",
    "sign_attention_masks = [data['sign_attention_mask'] for data in raw_data]\n",
    "eeg_attention_masks = [data['eeg_attention_mask'] for data in raw_data]\n",
    "sign_cls_tokens = [data['sign_cls_token'] for data in raw_data]\n",
    "eeg_cls_tokens = [data['eeg_cls_token'] for data in raw_data]\n",
    "labels = [data['label'] for data in raw_data]\n",
    "files = [data['file'] for data in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794909ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sign data type: \", type(sign_data))\n",
    "print(\"EEG data type: \", type(eeg_data))\n",
    "print(\"Sign attention mask type: \", type(sign_attention_masks[0]))\n",
    "print(\"EEG attention mask type: \", type(eeg_attention_masks[0]))\n",
    "print(\"Sign cls token type: \", type(sign_cls_tokens[0]))\n",
    "print(\"EEG cls token type: \", type(eeg_cls_tokens[0]))\n",
    "print(\"Labels type: \", type(labels))\n",
    "print(\"Files type: \", type(files))\n",
    "\n",
    "\n",
    "print(\"Sign data shape: \", sign_data[0].shape)\n",
    "print(\"EEG data shape: \", eeg_data[0].shape)\n",
    "print(\"Sign attention mask shape: \", sign_attention_masks[0].shape)\n",
    "print(\"EEG attention mask shape: \", eeg_attention_masks[0].shape)\n",
    "print(\"Sign cls token shape: \", sign_cls_tokens[0].shape)\n",
    "print(\"EEG cls token shape: \", eeg_cls_tokens[0].shape)\n",
    "print(\"Labels shape: \", len(labels))\n",
    "print(\"Files shape: \", len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c6a5e",
   "metadata": {},
   "source": [
    "## Redesigning the Transformer model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureEEGDataset(Dataset):\n",
    "    def __init__(self, input_data, num_classes):\n",
    "        sign_data = input_data['sign_data']\n",
    "        eeg_data = input_data['eeg_data']\n",
    "        sign_attention_masks = input_data['sign_attention_masks']\n",
    "        eeg_attention_masks = input_data['eeg_attention_masks']\n",
    "        sign_cls_tokens = input_data['sign_cls_tokens']\n",
    "        eeg_cls_tokens = input_data['eeg_cls_tokens']\n",
    "        labels = input_data['labels']\n",
    "\n",
    "        self.sign_x_ts = sign_data\n",
    "        self.sign_cls_token = sign_cls_tokens\n",
    "        self.sign_attention_mask = sign_attention_masks\n",
    "        self.sign_seq_len = sign_data[0].shape[0]\n",
    "        self.sign_ts_dim = sign_data[0].shape[1]\n",
    "\n",
    "        self.eeg_x_ts = eeg_data\n",
    "        self.eeg_cls_token = eeg_cls_tokens\n",
    "        self.eeg_attention_mask = eeg_attention_masks\n",
    "        self.eeg_seq_len = eeg_data[0].shape[0]\n",
    "        self.eeg_ts_dim = eeg_data[0].shape[1]\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sign_x_ts': self.sign_x_ts[idx],\n",
    "            'sign_cls_token': self.sign_cls_token[idx],\n",
    "            'sign_attention_mask': self.sign_attention_mask[idx],\n",
    "            'eeg_x_ts': self.eeg_x_ts[idx],\n",
    "            'eeg_cls_token': self.eeg_cls_token[idx],\n",
    "            'eeg_attention_mask': self.eeg_attention_mask[idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61162a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len + 1, d_model)\n",
    "        position = torch.arange(0, max_len + 1, dtype = torch.float).unsqueeze(1)\n",
    "        divterm = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # Credits to hkproj@github for this as https://github.com/hkproj/pytorch-transformer/blob/main/model.py\n",
    "        pe[:, 0::2] = torch.sin(position * divterm)\n",
    "        pe[:, 1::2] = torch.cos(position * divterm)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d21068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureEEGTransformer(nn.Module):\n",
    "    def __init__(self, sign_input_dim, sign_cls_dim, eeg_input_dim, eeg_cls_dim, d_model, num_classes, num_heads, num_layers, sign_max_seq_len, eeg_max_seq_len, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.sign_transfomer = SignatureTransformer(sign_input_dim, sign_cls_dim, d_model, num_classes, num_heads, num_layers, sign_max_seq_len, dropout)\n",
    "        self.eeg_transformer = SignatureTransformer(eeg_input_dim, eeg_cls_dim, d_model, num_classes, num_heads, num_layers, eeg_max_seq_len, dropout)\n",
    "\n",
    "        self.classifier = nn.Sequential(nn.Linear(d_model * 2, d_model), nn.ReLU(), nn.Linear(d_model, num_classes))\n",
    "\n",
    "    def forward(self, sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attn_mask = None, eeg_attn_mask = None):\n",
    "        sign_cls = self.sign_transfomer(sign_x_ts, sign_cls_token, sign_attn_mask)\n",
    "        eeg_cls = self.eeg_transformer(eeg_x_ts, eeg_cls_token, eeg_attn_mask)\n",
    "        multimodal_cls_output = torch.cat([sign_cls, eeg_cls], dim = 1)\n",
    "\n",
    "        logits = self.classifier(multimodal_cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88540e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    'sign_data': sign_data,\n",
    "    'eeg_data': eeg_data,\n",
    "    'sign_attention_masks': sign_attention_masks,\n",
    "    'eeg_attention_masks': eeg_attention_masks,\n",
    "    'sign_cls_tokens': sign_cls_tokens,\n",
    "    'eeg_cls_tokens': eeg_cls_tokens,\n",
    "    'labels': labels,\n",
    "}\n",
    "num_classes = 2\n",
    "multimodal_dataset = SignatureEEGDataset(input_data, num_classes)\n",
    "batch_size = 16\n",
    "multimodal_dataloader = DataLoader(multimodal_dataset, batch_size=batch_size, shuffle=True)\n",
    "sign_ts_dim = input_data['sign_data'][0].size(1)\n",
    "sign_cls_dim = input_data['sign_cls_tokens'][0].size(0)\n",
    "sign_seq_len = input_data['sign_data'][0].size(0)\n",
    "eeg_ts_dim = input_data['eeg_data'][0].size(1)\n",
    "eeg_cls_dim = input_data['eeg_cls_tokens'][0].size(0)\n",
    "eeg_seq_len = input_data['eeg_data'][0].size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer training loop\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = SignatureEEGTransformer(sign_input_dim=sign_ts_dim, sign_cls_dim=sign_cls_dim, eeg_input_dim=eeg_ts_dim, eeg_cls_dim=eeg_cls_dim, d_model=128, num_classes=2, num_heads=4, num_layers=2, sign_max_seq_len=sign_seq_len, eeg_max_seq_len=eeg_seq_len).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# num_epochs = 5\n",
    "\n",
    "labels_np = labels.cpu().numpy() if isinstance(labels, torch.Tensor) and labels.is_cuda else (\n",
    "    labels.numpy() if isinstance(labels, torch.Tensor) else labels\n",
    ")\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(labels)), test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "train_data = [raw_data[i] for i in train_indices]\n",
    "val_data = [raw_data[i] for i in val_indices]\n",
    "\n",
    "def build_input(data):\n",
    "    return {\n",
    "        'sign_data': [d['sign_data'] for d in data],\n",
    "        'eeg_data': [d['eeg_data'] for d in data],\n",
    "        'sign_attention_masks': [d['sign_attention_mask'] for d in data],\n",
    "        'eeg_attention_masks': [d['eeg_attention_mask'] for d in data],\n",
    "        'sign_cls_tokens': [d['sign_cls_token'] for d in data],\n",
    "        'eeg_cls_tokens': [d['eeg_cls_token'] for d in data],\n",
    "        'labels': [d['label'] for d in data],\n",
    "    }\n",
    "\n",
    "train_input = build_input(train_data)\n",
    "val_input = build_input(val_data)\n",
    "\n",
    "train_dataset = SignatureEEGDataset(train_input, num_classes=2)\n",
    "val_dataset = SignatureEEGDataset(val_input, num_classes=2)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SignatureEEGTransformer(\n",
    "    sign_input_dim=sign_ts_dim, sign_cls_dim=sign_cls_dim,\n",
    "    eeg_input_dim=eeg_ts_dim, eeg_cls_dim=eeg_cls_dim,\n",
    "    d_model=128, num_classes=2, num_heads=4, num_layers=2,\n",
    "    sign_max_seq_len=sign_seq_len, eeg_max_seq_len=eeg_seq_len\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_labels, train_preds = 0, [], []\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
    "        sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "        sign_cls_token = batch['sign_cls_token'].to(device)\n",
    "        sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "        eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "        eeg_cls_token = batch['eeg_cls_token'].to(device)\n",
    "        eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attention_mask, eeg_attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_labels)\n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_labels, val_preds = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False):\n",
    "            sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "            sign_cls_token = batch['sign_cls_token'].to(device)\n",
    "            sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "            eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "            eeg_cls_token = batch['eeg_cls_token'].to(device)\n",
    "            eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attention_mask, eeg_attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_labels)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "torch.save(model.state_dict(), os.path.join(os.getenv(\"MODEL_PATH\"), f\"multimodal_model_{datetime.now().strftime('%m%d%Y-%H%M%S')}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf85013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation loop\n",
    "# Set model to evaluation mode\n",
    "\n",
    "# Prepare test data\n",
    "files_mat_test, user_ids_test, test_labels = get_dataset_files_and_user_ids(data_category=constants.ALL, data_type=constants.TEST)\n",
    "raw_test_data = get_sig_eeg_raw_data(files_mat_test, test_labels)\n",
    "\n",
    "# Augment and preprocess test data (no augmentation, just features and padding)\n",
    "for i in range(len(raw_test_data)):\n",
    "    sign_data_with_features, sign_cls_token = get_sign_data_features(raw_test_data[i]['sign_data'])\n",
    "    eeg_data_with_features, eeg_cls_token = get_eeg_data_features(raw_test_data[i]['eeg_data'])\n",
    "    raw_test_data[i]['sign_data'] = sign_data_with_features\n",
    "    raw_test_data[i]['sign_cls_token'] = sign_cls_token\n",
    "    raw_test_data[i]['eeg_data'] = eeg_data_with_features\n",
    "    raw_test_data[i]['eeg_cls_token'] = eeg_cls_token\n",
    "\n",
    "sign_max_seq_len_test = max_seq_len_for_data\n",
    "eeg_max_seq_len_test = 10\n",
    "\n",
    "for i in range(len(raw_test_data)):\n",
    "    sign_data = raw_test_data[i]['sign_data']\n",
    "    eeg_data = raw_test_data[i]['eeg_data']\n",
    "    sign_data, sign_attention_mask = attach_attention_tokens_and_padding(sign_data, sign_max_seq_len_test)\n",
    "    eeg_data, eeg_attention_mask = attach_attention_tokens_and_padding(eeg_data, eeg_max_seq_len_test)\n",
    "    raw_test_data[i]['sign_data'] = sign_data\n",
    "    raw_test_data[i]['eeg_data'] = eeg_data\n",
    "    raw_test_data[i]['sign_attention_mask'] = sign_attention_mask\n",
    "    raw_test_data[i]['eeg_attention_mask'] = eeg_attention_mask\n",
    "\n",
    "test_input_data = {\n",
    "    'sign_data': [data['sign_data'] for data in raw_test_data],\n",
    "    'eeg_data': [data['eeg_data'] for data in raw_test_data],\n",
    "    'sign_attention_masks': [data['sign_attention_mask'] for data in raw_test_data],\n",
    "    'eeg_attention_masks': [data['eeg_attention_mask'] for data in raw_test_data],\n",
    "    'sign_cls_tokens': [data['sign_cls_token'] for data in raw_test_data],\n",
    "    'eeg_cls_tokens': [data['eeg_cls_token'] for data in raw_test_data],\n",
    "    'labels': [data['label'] for data in raw_test_data],\n",
    "}\n",
    "\n",
    "sign_ts_dim = test_input_data['sign_data'][0].size(1)\n",
    "sign_cls_dim = test_input_data['sign_cls_tokens'][0].size(0)\n",
    "sign_seq_len = test_input_data['sign_data'][0].size(0)\n",
    "eeg_ts_dim = test_input_data['eeg_data'][0].size(1)\n",
    "eeg_cls_dim = test_input_data['eeg_cls_tokens'][0].size(0)\n",
    "eeg_seq_len = test_input_data['eeg_data'][0].size(0)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_loss = 0\n",
    "val_labels = []\n",
    "val_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "        sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "        sign_cls_token = batch['sign_cls_token'].to(device)\n",
    "        sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "        eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "        eeg_cls_token = batch['eeg_cls_token'].to(device)\n",
    "        eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        logits = model(sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attention_mask, eeg_attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        val_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "avg_val_loss = val_loss / len(val_labels)\n",
    "val_acc = accuracy_score(val_labels, val_preds)\n",
    "val_prec = precision_score(val_labels, val_preds, zero_division=0)\n",
    "val_rec = recall_score(val_labels, val_preds, zero_division=0)\n",
    "val_f1 = f1_score(val_labels, val_preds, zero_division=0)\n",
    "\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Precision: {val_prec:.4f}\")\n",
    "print(f\"Validation Recall: {val_rec:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4667389",
   "metadata": {},
   "source": [
    "# Sign + EEG anomaly detection - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abcc9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDataset(Dataset):\n",
    "    def __init__(self, input_data, num_classes):\n",
    "        sign_data = input_data['sign_data']\n",
    "        eeg_data = input_data['eeg_data']\n",
    "        sign_attention_masks = input_data['sign_attention_masks']\n",
    "        eeg_attention_masks = input_data['eeg_attention_masks']\n",
    "        # sign_cls_tokens = input_data['sign_cls_tokens']\n",
    "        # eeg_cls_tokens = input_data['eeg_cls_tokens']\n",
    "        labels = input_data['labels']\n",
    "\n",
    "        self.sign_x_ts = sign_data\n",
    "        # self.sign_cls_token = sign_cls_tokens\n",
    "        self.sign_attention_mask = sign_attention_masks\n",
    "        self.sign_seq_len = sign_data[0].shape[0]\n",
    "        self.sign_ts_dim = sign_data[0].shape[1]\n",
    "\n",
    "        self.eeg_x_ts = eeg_data\n",
    "        # self.eeg_cls_token = eeg_cls_tokens\n",
    "        self.eeg_attention_mask = eeg_attention_masks\n",
    "        self.eeg_seq_len = eeg_data[0].shape[0]\n",
    "        self.eeg_ts_dim = eeg_data[0].shape[1]\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sign_x_ts': self.sign_x_ts[idx],\n",
    "            # 'sign_cls_token': self.sign_cls_token[idx],\n",
    "            'sign_attention_mask': self.sign_attention_mask[idx],\n",
    "            'eeg_x_ts': self.eeg_x_ts[idx],\n",
    "            # 'eeg_cls_token': self.eeg_cls_token[idx],\n",
    "            'eeg_attention_mask': self.eeg_attention_mask[idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e5a3179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_2588\\2339106861.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  normalized_sign_data = torch.tensor(normalized_sign_data, dtype=torch.float32)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_2588\\4004042047.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 136, using nperseg = 136\n",
      "  f, Pxx = welch(normalized_signal[:, ch].cpu().numpy(), fs=fs)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_2588\\4004042047.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 197, using nperseg = 197\n",
      "  f, Pxx = welch(normalized_signal[:, ch].cpu().numpy(), fs=fs)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_2588\\4004042047.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 203, using nperseg = 203\n",
      "  f, Pxx = welch(normalized_signal[:, ch].cpu().numpy(), fs=fs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files_mat_genuine_train, user_ids_genuine_train, train_labels = get_dataset_files_and_user_ids(data_category=constants.GENUINE, data_type=constants.TRAIN)\n",
    "files_mat_test, user_ids_test, test_labels = get_dataset_files_and_user_ids(data_category=constants.ALL, data_type=constants.TEST)\n",
    "\n",
    "raw_data_train = get_sig_eeg_raw_data(files_mat_genuine_train, train_labels)\n",
    "raw_data_test = get_sig_eeg_raw_data(files_mat_test, test_labels)\n",
    "\n",
    "sign_max_seq_len = max_seq_len_for_data\n",
    "eeg_max_seq_len = int(max_seq_len_for_data // 2)\n",
    "\n",
    "for i in range(len(raw_data_train)):\n",
    "    sign_feat, _ = get_sign_data_features(raw_data_train[i]['sign_data'])\n",
    "    eeg_data, _, _ = get_eeg_data_features(raw_data_train[i]['eeg_data'])\n",
    "    sign_feat, sign_mask = attach_attention_tokens_and_padding(sign_feat, sign_max_seq_len)\n",
    "    eeg_data, eeg_mask = attach_attention_tokens_and_padding(eeg_data, eeg_max_seq_len)\n",
    "    raw_data_train[i]['sign_data'] = sign_feat\n",
    "    raw_data_train[i]['eeg_data'] = eeg_data\n",
    "    raw_data_train[i]['sign_attention_mask'] = sign_mask\n",
    "    raw_data_train[i]['eeg_attention_mask'] = eeg_mask\n",
    "\n",
    "for i in range(len(raw_data_test)):\n",
    "    sign_feat, _ = get_sign_data_features(raw_data_test[i]['sign_data'])\n",
    "    eeg_data, _, _ = get_eeg_data_features(raw_data_test[i]['eeg_data'])\n",
    "    sign_feat, sign_mask = attach_attention_tokens_and_padding(sign_feat, sign_max_seq_len)\n",
    "    eeg_data, eeg_mask = attach_attention_tokens_and_padding(eeg_data, eeg_max_seq_len)\n",
    "    raw_data_test[i]['sign_data'] = sign_feat\n",
    "    raw_data_test[i]['eeg_data'] = eeg_data\n",
    "    raw_data_test[i]['sign_attention_mask'] = sign_mask\n",
    "    raw_data_test[i]['eeg_attention_mask'] = eeg_mask\n",
    "\n",
    "sign_data_train = [d['sign_data'] for d in raw_data_train]\n",
    "eeg_data_train = [d['eeg_data'] for d in raw_data_train]\n",
    "sign_attention_masks_train = [d['sign_attention_mask'][1:] for d in raw_data_train]\n",
    "eeg_attention_masks_train = [d['eeg_attention_mask'][1:] for d in raw_data_train]\n",
    "labels_train = [d['label'] for d in raw_data_train]\n",
    "\n",
    "sign_data_test = [d['sign_data'] for d in raw_data_test]\n",
    "eeg_data_test = [d['eeg_data'] for d in raw_data_test]\n",
    "sign_attention_masks_test = [d['sign_attention_mask'][1:] for d in raw_data_test]\n",
    "eeg_attention_masks_test = [d['eeg_attention_mask'][1:] for d in raw_data_test]\n",
    "labels_test = [d['label'] for d in raw_data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7bf5795",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AnomalyDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m input_data_train = {\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msign_data\u001b[39m\u001b[33m'\u001b[39m: sign_data_train,\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meeg_data\u001b[39m\u001b[33m'\u001b[39m: eeg_data_train,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: labels_train,\n\u001b[32m      7\u001b[39m }\n\u001b[32m      8\u001b[39m input_data_test = {\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msign_data\u001b[39m\u001b[33m'\u001b[39m: sign_data_test,\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33meeg_data\u001b[39m\u001b[33m'\u001b[39m: eeg_data_test,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: labels_test,\n\u001b[32m     14\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m anomaly_train_dataset = \u001b[43mAnomalyDataset\u001b[49m(input_data_train, num_classes=\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m anomaly_test_dataset = AnomalyDataset(input_data_test, num_classes=\u001b[32m1\u001b[39m)\n\u001b[32m     17\u001b[39m anomaly_train_loader = DataLoader(anomaly_train_dataset, batch_size=\u001b[32m16\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'AnomalyDataset' is not defined"
     ]
    }
   ],
   "source": [
    "input_data_train = {\n",
    "    'sign_data': sign_data_train,\n",
    "    'eeg_data': eeg_data_train,\n",
    "    'sign_attention_masks': sign_attention_masks_train,\n",
    "    'eeg_attention_masks': eeg_attention_masks_train,\n",
    "    'labels': labels_train,\n",
    "}\n",
    "input_data_test = {\n",
    "    'sign_data': sign_data_test,\n",
    "    'eeg_data': eeg_data_test,\n",
    "    'sign_attention_masks': sign_attention_masks_test,\n",
    "    'eeg_attention_masks': eeg_attention_masks_test,\n",
    "    'labels': labels_test,\n",
    "}\n",
    "anomaly_train_dataset = AnomalyDataset(input_data_train, num_classes=1)\n",
    "anomaly_test_dataset = AnomalyDataset(input_data_test, num_classes=1)\n",
    "anomaly_train_loader = DataLoader(anomaly_train_dataset, batch_size=16, shuffle=True)\n",
    "anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=16, shuffle=False)\n",
    "sign_ts_dim_train = input_data_train['sign_data'][0].size(1)\n",
    "# sign_attention_masks_train = input_data_train['sign_attention_masks'][0].size(0)\n",
    "sign_seq_len_train = input_data_train['sign_data'][0].size(0)\n",
    "eeg_ts_dim_train = input_data_train['eeg_data'][0].size(1)\n",
    "# eeg_attention_masks_train = input_data_train['eeg_attention_masks'][0].size(0)\n",
    "eeg_seq_len_train = input_data_train['eeg_data'][0].size(0)\n",
    "\n",
    "sign_ts_dim_test = input_data_test['sign_data'][0].size(1)\n",
    "# sign_attention_masks_test = input_data_test['sign_attention_masks'][0].size(0)\n",
    "sign_seq_len_test = input_data_test['sign_data'][0].size(0)\n",
    "eeg_ts_dim_test = input_data_test['eeg_data'][0].size(1)\n",
    "# eeg_attention_masks_test = input_data_test['eeg_attention_masks'][0].size(0)\n",
    "eeg_seq_len_test = input_data_test['eeg_data'][0].size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c660ee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign data shape:  torch.Size([3000, 11])\n",
      "EEG data shape:  torch.Size([1500, 5])\n",
      "Sign attention mask shape:  torch.Size([3000])\n",
      "EEG attention mask shape:  torch.Size([1500])\n",
      "Labels shape:  582\n",
      "Labels unique elements:  {0}\n"
     ]
    }
   ],
   "source": [
    "# print(\"Sign data type: \", eeg_data_train[0])\n",
    "# print(\"EEG data type: \", type(eeg_data))\n",
    "# print(\"Sign attention mask type: \", type(sign_attention_masks[0]))\n",
    "# print(\"EEG attention mask type: \", type(eeg_attention_masks[0]))\n",
    "# print(\"Sign cls token type: \", type(sign_cls_tokens[0]))\n",
    "# print(\"EEG cls token type: \", type(eeg_cls_tokens[0]))\n",
    "# print(\"Labels type: \", type(labels))\n",
    "# print(\"Files type: \", type(files))\n",
    "\n",
    "\n",
    "print(\"Sign data shape: \", sign_data_train[0].shape)\n",
    "print(\"EEG data shape: \", eeg_data_train[0].shape)\n",
    "print(\"Sign attention mask shape: \", sign_attention_masks_train[0].shape)\n",
    "print(\"EEG attention mask shape: \", eeg_attention_masks_train[0].shape)\n",
    "# print(\"Sign cls token shape: \", sign_cls_tokens_train[0].shape)\n",
    "# print(\"EEG cls token shape: \", eeg_cls_tokens_train[0].shape)\n",
    "print(\"Labels shape: \", len(labels_train))\n",
    "print(\"Labels unique elements: \", set(labels_train))\n",
    "# print(\"Files shape: \", len(files_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9220ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.isnan(sign_x_ts).any(), torch.isinf(sign_x_ts).any())\n",
    "# print(torch.isnan(eeg_x_ts).any(), torch.isinf(eeg_x_ts).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6136abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignEEGTransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, sign_input_dim, sign_seq_len, eeg_input_dim, eeg_seq_len, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sign_input_proj = nn.Linear(sign_input_dim, d_model)\n",
    "        self.sign_pos_enc = PositionalEncoding(d_model, sign_seq_len)\n",
    "        sign_encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.sign_encoder = nn.TransformerEncoder(sign_encoder_layer, num_layers=num_layers)\n",
    "        sign_decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.sign_decoder = nn.TransformerDecoder(sign_decoder_layer, num_layers=num_layers)\n",
    "        self.sign_output_proj = nn.Linear(d_model, sign_input_dim)\n",
    "\n",
    "        self.eeg_input_proj = nn.Linear(eeg_input_dim, d_model)\n",
    "        self.eeg_pos_enc = PositionalEncoding(d_model, eeg_seq_len)\n",
    "        eeg_encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.eeg_encoder = nn.TransformerEncoder(eeg_encoder_layer, num_layers=num_layers)\n",
    "        eeg_decoder_layer = nn.TransformerDecoderLayer(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.eeg_decoder = nn.TransformerDecoder(eeg_decoder_layer, num_layers=num_layers)\n",
    "        self.eeg_output_proj = nn.Linear(d_model, eeg_input_dim)\n",
    "\n",
    "    def forward(self, sign_x, eeg_x, sign_attn_mask=None, eeg_attn_mask=None):\n",
    "        sign_x_proj = self.sign_input_proj(sign_x) + self.sign_pos_enc(sign_x)\n",
    "        sign_memory = self.sign_encoder(sign_x_proj, src_key_padding_mask=sign_attn_mask)\n",
    "        sign_decoded = self.sign_decoder(sign_x_proj, sign_memory, tgt_key_padding_mask=sign_attn_mask, memory_key_padding_mask=sign_attn_mask)\n",
    "        sign_recon = self.sign_output_proj(sign_decoded)\n",
    "        eeg_x_proj = self.eeg_input_proj(eeg_x) + self.eeg_pos_enc(eeg_x)\n",
    "        eeg_memory = self.eeg_encoder(eeg_x_proj, src_key_padding_mask=eeg_attn_mask)\n",
    "        eeg_decoded = self.eeg_decoder(eeg_x_proj, eeg_memory, tgt_key_padding_mask=eeg_attn_mask, memory_key_padding_mask=eeg_attn_mask)\n",
    "        eeg_recon = self.eeg_output_proj(eeg_decoded)\n",
    "\n",
    "        return sign_recon, eeg_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33023d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_autoencoder(model, train_loader, val_loader, device, num_epochs=20, lr=1e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
    "            sign_x = batch['sign_x_ts'].to(device)\n",
    "            sign_mask = batch['sign_attention_mask'].to(device) if 'sign_attention_mask' in batch else None\n",
    "            if sign_mask.dim() == 3:\n",
    "                sign_mask = sign_mask.squeeze(1) if sign_mask is not None else None\n",
    "            eeg_x = batch['eeg_x_ts'].to(device)\n",
    "            eeg_mask = batch['eeg_attention_mask'].to(device) if 'eeg_attention_mask' in batch else None\n",
    "            if eeg_mask.dim() == 3:\n",
    "                eeg_mask = eeg_mask.squeeze(1) if eeg_mask is not None else None\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            sign_recon, eeg_recon = model(sign_x, eeg_x, sign_mask, eeg_mask)\n",
    "            loss_sign = loss_fn(sign_recon, sign_x)\n",
    "            loss_eeg = loss_fn(eeg_recon, eeg_x)\n",
    "            loss = loss_sign + loss_eeg\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False):\n",
    "                sign_x = batch['sign_x_ts'].to(device)\n",
    "                sign_mask = batch['sign_attention_mask'].to(device) if 'sign_attention_mask' in batch else None\n",
    "                if sign_mask.dim() == 3:\n",
    "                    sign_mask = sign_mask.squeeze(1) if sign_mask is not None else None \n",
    "                eeg_x = batch['eeg_x_ts'].to(device)\n",
    "                eeg_mask = batch['eeg_attention_mask'].to(device) if 'eeg_attention_mask' in batch else None\n",
    "                if eeg_mask.dim() == 3:\n",
    "                    eeg_mask = eeg_mask.squeeze(1) if eeg_mask is not None else None\n",
    "\n",
    "                sign_recon, eeg_recon = model(sign_x, eeg_x, sign_mask, eeg_mask)\n",
    "                loss_sign = loss_fn(sign_recon, sign_x)\n",
    "                loss_eeg = loss_fn(eeg_recon, eeg_x)\n",
    "                loss = loss_sign + loss_eeg\n",
    "                val_loss += loss.item() \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "anomaly_model = SignEEGTransformerAutoEncoder(sign_input_dim=sign_ts_dim_train, sign_seq_len=sign_seq_len_train, eeg_input_dim=eeg_ts_dim_train, eeg_seq_len=eeg_seq_len_train, d_model=128, num_heads=4, num_layers=2).to(device)\n",
    "\n",
    "model = train_validate_autoencoder(anomaly_model, anomaly_train_loader, anomaly_test_loader, device, num_epochs=10, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abbc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_8784\\991432957.py:10: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  model.load_state_dict(torch.load(\"D:\\\\KCL Final Year Individual Project\\\\Implementation\\\\Project Implementation\\models\\\\anomaly_model_07212025-153342.pth\", map_location='cpu'))\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_8784\\991432957.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['labels'], dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.5362\n",
      "Accuracy: 0.5778\n",
      "Precision: 0.6800\n",
      "Recall: 0.1910\n",
      "F1 Score: 0.2982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Genuine       0.56      0.92      0.70       201\n",
      "     Anomaly       0.68      0.19      0.30       178\n",
      "\n",
      "    accuracy                           0.58       379\n",
      "   macro avg       0.62      0.56      0.50       379\n",
      "weighted avg       0.62      0.58      0.51       379\n",
      "\n",
      "Confusion Matrix:\n",
      " [[185  16]\n",
      " [144  34]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = SignEEGTransformerAutoEncoder(\n",
    "#     sign_input_dim=sign_ts_dim_test,\n",
    "#     sign_seq_len=sign_seq_len_test,\n",
    "#     eeg_input_dim=eeg_ts_dim_test,\n",
    "#     eeg_seq_len=eeg_seq_len_test,\n",
    "#     d_model=128,\n",
    "#     num_heads=4,\n",
    "#     num_layers=2\n",
    "# )\n",
    "# model.load_state_dict(torch.load(\"D:\\\\KCL Final Year Individual Project\\\\Implementation\\\\Project Implementation\\models\\\\anomaly_model_07212025-153342.pth\", map_location='cpu'))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "all_scores = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in anomaly_test_loader:\n",
    "        sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "        eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "        sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "        eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "        labels = torch.tensor(batch['labels'], dtype=torch.float32).to(device)\n",
    "        sign_recon, eeg_recon = model(sign_x_ts, eeg_x_ts, sign_attention_mask, eeg_attention_mask)\n",
    "        sign_error = ((sign_recon - sign_x_ts) ** 2).max(dim=(1, 2)).cpu().numpy()\n",
    "        eeg_error = ((eeg_recon - eeg_x_ts) ** 2).max(dim=(1, 2)).cpu().numpy()\n",
    "        scores = sign_error + eeg_error\n",
    "        all_scores.extend(scores)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "all_scores_np = np.array(all_scores)\n",
    "all_labels_np = np.array(all_labels)\n",
    "fpr, tpr, thresholds = roc_curve(all_labels_np, all_scores_np)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "threshold = thresholds[optimal_idx]\n",
    "# print(\"Optimal threshold:\", optimal_threshold)\n",
    "preds = (np.array(all_scores) > threshold).astype(int)\n",
    "roc = roc_auc_score(all_labels, all_scores)\n",
    "acc = accuracy_score(all_labels, preds)\n",
    "prec = precision_score(all_labels, preds, zero_division=0)\n",
    "rec = recall_score(all_labels, preds, zero_division=0)\n",
    "f1 = f1_score(all_labels, preds, zero_division=0)\n",
    "\n",
    "print(f\"ROC AUC: {roc:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(classification_report(all_labels, preds, target_names=['Genuine', 'Anomaly'], labels=[0,1]))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, preds))\n",
    "\n",
    "plt.hist(np.array(all_scores)[np.array(all_labels)==0], bins=50, alpha=0.5, label='Genuine')\n",
    "plt.hist(np.array(all_scores)[np.array(all_labels)==1], bins=50, alpha=0.5, label='Anomaly')\n",
    "plt.legend()\n",
    "plt.title(\"Anomaly Scores Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca15cb9",
   "metadata": {},
   "source": [
    "## Sign + EEG Anomaly - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4f6e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyCNNDataset(Dataset):\n",
    "    def __init__(self, input_data):\n",
    "        sign_data = input_data['sign_data']\n",
    "        eeg_data = input_data['eeg_data']\n",
    "        labels = input_data['labels']\n",
    "\n",
    "        self.sign_x_ts = sign_data\n",
    "        # # self.sign_cls_token = sign_cls_tokens\n",
    "        # self.sign_attention_mask = sign_attention_masks\n",
    "        self.sign_seq_len = sign_data[0].shape[0]\n",
    "        self.sign_ts_dim = sign_data[0].shape[1]\n",
    "\n",
    "        self.eeg_x_ts = eeg_data\n",
    "        # # self.eeg_cls_token = eeg_cls_tokens\n",
    "        # self.eeg_attention_mask = eeg_attention_masks\n",
    "        self.eeg_seq_len = eeg_data[0].shape[0]\n",
    "        self.eeg_ts_dim = eeg_data[0].shape[1]\n",
    "\n",
    "        # self.num_classes = num_classes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sign_x_ts': self.sign_x_ts[idx],\n",
    "            # # 'sign_cls_token': self.sign_cls_token[idx],\n",
    "            # 'sign_attention_mask': self.sign_attention_mask[idx],\n",
    "            'eeg_x_ts': self.eeg_x_ts[idx],\n",
    "            # # 'eeg_cls_token': self.eeg_cls_token[idx],\n",
    "            # 'eeg_attention_mask': self.eeg_attention_mask[idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cf77fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, latent_dim, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        z = self.encoder(x)\n",
    "        return z.squeeze(-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "625de453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32 * (seq_len // 4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (32, seq_len // 4)),\n",
    "            nn.ConvTranspose1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(64, output_dim, kernel_size=4, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.decoder(z)\n",
    "        return x.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48c33ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignEEGCNNAutoencoder(nn.Module):\n",
    "    def __init__(self, sign_input_dim, eeg_input_dim, sign_seq_len, eeg_seq_len, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.sign_encoder = CNNEncoder(sign_input_dim, latent_dim)\n",
    "        self.eeg_encoder = CNNEncoder(eeg_input_dim, latent_dim)\n",
    "        self.sign_decoder = CNNDecoder(latent_dim, sign_input_dim, sign_seq_len)\n",
    "        self.eeg_decoder = CNNDecoder(latent_dim, eeg_input_dim, eeg_seq_len)\n",
    "\n",
    "    def forward(self, sign_x, eeg_x):\n",
    "        sign_z = self.sign_encoder(sign_x)\n",
    "        eeg_z = self.eeg_encoder(eeg_x)\n",
    "        sign_recon = self.sign_decoder(sign_z)\n",
    "        eeg_recon = self.eeg_decoder(eeg_z)\n",
    "        return sign_recon, eeg_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8d50f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_autoencoder(model, train_loader, device, num_epochs=10, lr=1e-4):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
    "            # print(\"Current batch sign seq len: \", batch['sign_x_ts'].shape[1])\n",
    "            # print(\"Current batch eeg seq len: \", batch['eeg_x_ts'].shape[1])\n",
    "            sign_x = batch['sign_x_ts'].to(device)\n",
    "            # sign_mask = batch['sign_attention_mask'].to(device) if 'sign_attention_mask' in batch else None\n",
    "            # if sign_mask.dim() == 3:\n",
    "            #     sign_mask = sign_mask.squeeze(1) if sign_mask is not None else None\n",
    "            eeg_x = batch['eeg_x_ts'].to(device)\n",
    "            # eeg_mask = batch['eeg_attention_mask'].to(device) if 'eeg_attention_mask' in batch else None\n",
    "            # if eeg_mask.dim() == 3:\n",
    "            #     eeg_mask = eeg_mask.squeeze(1) if eeg_mask is not None else None\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            sign_recon, eeg_recon = model(sign_x, eeg_x)\n",
    "            loss_sign = loss_fn(sign_recon, sign_x)\n",
    "            loss_eeg = loss_fn(eeg_recon, eeg_x)\n",
    "            loss = loss_sign + loss_eeg\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # model.eval()\n",
    "        # val_loss = 0\n",
    "        # with torch.no_grad():\n",
    "        #     for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False):\n",
    "        #         sign_x = batch['sign_x_ts'].to(device)\n",
    "        #         # sign_mask = batch['sign_attention_mask'].to(device) if 'sign_attention_mask' in batch else None\n",
    "        #         # if sign_mask.dim() == 3:\n",
    "        #         #     sign_mask = sign_mask.squeeze(1) if sign_mask is not None else None \n",
    "        #         eeg_x = batch['eeg_x_ts'].to(device)\n",
    "        #         # eeg_mask = batch['eeg_attention_mask'].to(device) if 'eeg_attention_mask' in batch else None\n",
    "        #         # if eeg_mask.dim() == 3:\n",
    "        #         #     eeg_mask = eeg_mask.squeeze(1) if eeg_mask is not None else None\n",
    "\n",
    "        #         sign_recon, eeg_recon = model(sign_x, eeg_x)\n",
    "        #         loss_sign = loss_fn(sign_recon, sign_x)\n",
    "        #         loss_eeg = loss_fn(eeg_recon, eeg_x)\n",
    "        #         loss = loss_sign + loss_eeg\n",
    "        #         val_loss += loss.item() \n",
    "        # avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        # print(f\"Epoch {epoch+1} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65dfab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_2588\\2339106861.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  normalized_sign_data = torch.tensor(normalized_sign_data, dtype=torch.float32)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_2588\\4004042047.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 136, using nperseg = 136\n",
      "  f, Pxx = welch(normalized_signal[:, ch].cpu().numpy(), fs=fs)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_2588\\4004042047.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 197, using nperseg = 197\n",
      "  f, Pxx = welch(normalized_signal[:, ch].cpu().numpy(), fs=fs)\n",
      "C:\\Users\\Abhay Nambiar\\AppData\\Local\\Temp\\ipykernel_2588\\4004042047.py:30: UserWarning: nperseg=256 is greater than signal length max(len(x), len(y)) = 203, using nperseg = 203\n",
      "  f, Pxx = welch(normalized_signal[:, ch].cpu().numpy(), fs=fs)\n"
     ]
    }
   ],
   "source": [
    "files_mat_genuine_train, user_ids_genuine_train, train_labels = get_dataset_files_and_user_ids(data_category=constants.GENUINE, data_type=constants.TRAIN)\n",
    "files_mat_test, user_ids_test, test_labels = get_dataset_files_and_user_ids(data_category=constants.ALL, data_type=constants.TEST)\n",
    "\n",
    "raw_data_train = get_sig_eeg_raw_data(files_mat_genuine_train, train_labels)\n",
    "raw_data_test = get_sig_eeg_raw_data(files_mat_test, test_labels)\n",
    "\n",
    "sign_max_seq_len = max_seq_len_for_data\n",
    "eeg_max_seq_len = int(max_seq_len_for_data // 2)\n",
    "\n",
    "for i in range(len(raw_data_train)):\n",
    "    sign_feat, sign_cls_token = get_sign_data_features(raw_data_train[i]['sign_data'])\n",
    "    eeg_data, eeg_features, eeg_cls_token = get_eeg_data_features(raw_data_train[i]['eeg_data'])\n",
    "    sign_cls_expanded = sign_cls_token.unsqueeze(0).expand(sign_data.shape[0], -1)\n",
    "    sign_data = torch.cat((sign_feat, sign_cls_expanded), dim=1)\n",
    "    eeg_cls_expanded = eeg_cls_token.unsqueeze(0).expand(eeg_data.shape[0], -1)\n",
    "    eeg_data = torch.cat([eeg_data, eeg_cls_expanded], dim=1)\n",
    "    raw_data_train[i]['sign_data'] = sign_feat\n",
    "    raw_data_train[i]['eeg_data'] = eeg_data\n",
    "\n",
    "for i in range(len(raw_data_test)):\n",
    "    sign_feat, _ = get_sign_data_features(raw_data_test[i]['sign_data'])\n",
    "    eeg_data, _, _ = get_eeg_data_features(raw_data_test[i]['eeg_data'])\n",
    "    raw_data_test[i]['sign_data'] = sign_feat\n",
    "    raw_data_test[i]['eeg_data'] = eeg_data\n",
    "\n",
    "sign_data_train = [d['sign_data'] for d in raw_data_train]\n",
    "eeg_data_train = [d['eeg_data'] for d in raw_data_train]\n",
    "labels_train = [d['label'] for d in raw_data_train]\n",
    "\n",
    "sign_data_test = [d['sign_data'] for d in raw_data_test]\n",
    "eeg_data_test = [d['eeg_data'] for d in raw_data_test]\n",
    "labels_test = [d['label'] for d in raw_data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1ca5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_eeg_collate_fn(batch):\n",
    "    # Use your global fixed lengths\n",
    "    fixed_sign_len = max_seq_len_for_data\n",
    "    fixed_eeg_len = int(max_seq_len_for_data // 2)\n",
    "    sign_x = [item['sign_x_ts'] for item in batch]\n",
    "    eeg_x = [item['eeg_x_ts'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    # Pad each sample individually to fixed length\n",
    "    sign_x_padded = torch.stack([\n",
    "        torch.nn.functional.pad(x, (0, 0, 0, fixed_sign_len - x.shape[0]), mode='constant', value=0)\n",
    "        for x in sign_x\n",
    "    ])\n",
    "    eeg_x_padded = torch.stack([\n",
    "        torch.nn.functional.pad(x, (0, 0, 0, fixed_eeg_len - x.shape[0]), mode='constant', value=0)\n",
    "        for x in eeg_x\n",
    "    ])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return {\n",
    "        'sign_x_ts': sign_x_padded,\n",
    "        'eeg_x_ts': eeg_x_padded,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c01a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_train = {\n",
    "    'sign_data': sign_data_train,\n",
    "    'eeg_data': eeg_data_train,\n",
    "    # 'sign_attention_masks': sign_attention_masks_train,\n",
    "    # 'eeg_attention_masks': eeg_attention_masks_train,\n",
    "    'labels': labels_train,\n",
    "}\n",
    "input_data_test = {\n",
    "    'sign_data': sign_data_test,\n",
    "    'eeg_data': eeg_data_test,\n",
    "    # 'sign_attention_masks': sign_attention_masks_test,\n",
    "    # 'eeg_attention_masks': eeg_attention_masks_test,\n",
    "    'labels': labels_test,\n",
    "}\n",
    "# anomaly_train_dataset = AnomalyCNNDataset(input_data_train)\n",
    "# anomaly_test_dataset = AnomalyCNNDataset(input_data_test)\n",
    "# anomaly_train_loader = DataLoader(anomaly_train_dataset, batch_size=16, shuffle=True, collate_fn=sign_collate_fn)\n",
    "# anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=16, shuffle=False, collate_fn=eeg_collate_fn)\n",
    "sign_ts_dim_train = input_data_train['sign_data'][0].size(1)\n",
    "# sign_attention_masks_train = input_data_train['sign_attention_masks'][0].size(0)\n",
    "sign_seq_len_train = input_data_train['sign_data'][0].size(0)\n",
    "eeg_ts_dim_train = input_data_train['eeg_data'][0].size(1)\n",
    "# eeg_attention_masks_train = input_data_train['eeg_attention_masks'][0].size(0)\n",
    "eeg_seq_len_train = input_data_train['eeg_data'][0].size(0)\n",
    "\n",
    "sign_ts_dim_test = input_data_test['sign_data'][0].size(1)\n",
    "# sign_attention_masks_test = input_data_test['sign_attention_masks'][0].size(0)\n",
    "sign_seq_len_test = input_data_test['sign_data'][0].size(0)\n",
    "eeg_ts_dim_test = input_data_test['eeg_data'][0].size(1)\n",
    "# eeg_attention_masks_test = input_data_test['eeg_attention_masks'][0].size(0)\n",
    "eeg_seq_len_test = input_data_test['eeg_data'][0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4a1ac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.2395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.2337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.2110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.1566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.1531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.1528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.1489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "sign_seq_len = sign_data_train[0].shape[0]\n",
    "sign_input_dim = sign_data_train[0].shape[1]\n",
    "eeg_seq_len = eeg_data_train[0].shape[0]\n",
    "eeg_input_dim = eeg_data_train[0].shape[1]\n",
    "latent_dim = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_dataset = AnomalyCNNDataset(input_data_train)\n",
    "test_dataset = AnomalyCNNDataset(input_data_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=sign_eeg_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=sign_eeg_collate_fn)\n",
    "\n",
    "anomaly_cnn_model = SignEEGCNNAutoencoder(sign_input_dim, eeg_input_dim, max_seq_len_for_data, int(max_seq_len_for_data // 2), latent_dim).to(device)\n",
    "anomaly_model = train_validate_autoencoder(anomaly_cnn_model, train_loader, device=device, num_epochs=10, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd0e7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 24/24 [00:00<00:00, 25.05it/s]\n"
     ]
    }
   ],
   "source": [
    "anomaly_model.eval()\n",
    "all_scores = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        sign_x = batch['sign_x_ts'].to(device)\n",
    "        eeg_x = batch['eeg_x_ts'].to(device)\n",
    "        labels = batch['labels'].cpu().numpy()\n",
    "        sign_recon, eeg_recon = anomaly_model(sign_x, eeg_x)\n",
    "        # using max squared error per sample as anomaly score\n",
    "        sign_error = torch.amax((sign_recon - sign_x) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "        eeg_error = torch.amax((eeg_recon - eeg_x) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "        scores = sign_error + eeg_error\n",
    "        all_scores.extend(scores)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "726ff6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.5732\n",
      "Accuracy: 0.5805\n",
      "Precision: 0.5888\n",
      "Recall: 0.3539\n",
      "F1 Score: 0.4421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Genuine       0.58      0.78      0.66       201\n",
      "     Anomaly       0.59      0.35      0.44       178\n",
      "\n",
      "    accuracy                           0.58       379\n",
      "   macro avg       0.58      0.57      0.55       379\n",
      "weighted avg       0.58      0.58      0.56       379\n",
      "\n",
      "Confusion Matrix:\n",
      " [[157  44]\n",
      " [115  63]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n",
    "roc_auc = roc_auc_score(all_labels, all_scores)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "threshold = thresholds[optimal_idx]\n",
    "preds = (all_scores > threshold).astype(int)\n",
    "\n",
    "acc = accuracy_score(all_labels, preds)\n",
    "prec = precision_score(all_labels, preds, zero_division=0)\n",
    "rec = recall_score(all_labels, preds, zero_division=0)\n",
    "f1 = f1_score(all_labels, preds, zero_division=0)\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(classification_report(all_labels, preds, target_names=['Genuine', 'Anomaly'], labels=[0,1]))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, preds))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(all_scores[all_labels==0], bins=50, alpha=0.5, label='Genuine')\n",
    "plt.hist(all_scores[all_labels==1], bins=50, alpha=0.5, label='Anomaly')\n",
    "plt.axvline(threshold, color='red', linestyle='--', label='Threshold')\n",
    "plt.legend()\n",
    "plt.title(\"Anomaly Scores Distribution\")\n",
    "plt.xlabel(\"Anomaly Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6020a",
   "metadata": {},
   "source": [
    "## Anomaly Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# num_epochs = 10\n",
    "\n",
    "# anomaly_model = SignatureEEGTransformer(\n",
    "#     sign_input_dim=sign_ts_dim, sign_cls_dim=sign_cls_dim,\n",
    "#     eeg_input_dim=eeg_ts_dim, eeg_cls_dim=eeg_cls_dim,\n",
    "#     d_model=128, num_classes=1, num_heads=4, num_layers=4,\n",
    "#     sign_max_seq_len=sign_seq_len, eeg_max_seq_len=eeg_seq_len\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(anomaly_model.parameters(), lr=1e-5)\n",
    "# loss_fn = nn.BCEWithLogitsLoss() # only one class, so only one logit output per sample\n",
    "\n",
    "# anomaly_test_dataset = SignatureEEGDataset(test_input_data, num_classes=1)\n",
    "# anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     anomaly_model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in tqdm(anomaly_train_dataloader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
    "#         sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "#         sign_cls_token = batch['sign_cls_token'].to(device)\n",
    "#         sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "#         eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "#         eeg_cls_token = batch['eeg_cls_token'].to(device)\n",
    "#         eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "#         labels = torch.zeros(sign_x_ts.size(0), dtype=torch.float32).to(device)  # All genuine = 0\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = anomaly_model(sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attention_mask, eeg_attention_mask)\n",
    "#         loss = loss_fn(logits.squeeze(1), labels)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(anomaly_model.parameters(), max_norm=1.0)\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item() * labels.size(0)\n",
    "#     avg_loss = total_loss / len(anomaly_train_dataloader.dataset)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     threshold = 0.5 \n",
    "#     # Validation/Test at every epoch\n",
    "#     anomaly_model.eval()\n",
    "#     all_scores = []\n",
    "#     all_labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in anomaly_test_loader:\n",
    "#             sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "#             sign_cls_token = batch['sign_cls_token'].to(device)\n",
    "#             sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "#             eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "#             eeg_cls_token = batch['eeg_cls_token'].to(device)\n",
    "#             eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "#             labels = torch.tensor(batch['labels'], dtype=torch.float32).to(device)\n",
    "#             scores = anomaly_model(sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attention_mask, eeg_attention_mask)\n",
    "#             all_scores.extend(scores.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#     preds = (np.array(all_scores) > threshold).astype(int)\n",
    "#     roc = roc_auc_score(all_labels, all_scores)\n",
    "#     print(f\"Validation ROC AUC: {roc:.4f}\")\n",
    "#     print(classification_report(all_labels, preds, target_names=['Genuine', 'Anomaly'], labels=[0,1]))\n",
    "#     print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, preds))\n",
    "#     plt.hist(np.array(all_scores)[np.array(all_labels)==0], bins=50, alpha=0.5, label='Genuine')\n",
    "#     plt.hist(np.array(all_scores)[np.array(all_labels)==1], bins=50, alpha=0.5, label='Anomaly')\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155207fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training loop for anomaly detection\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# num_epochs = 5\n",
    "\n",
    "# anomaly_model = SignatureEEGTransformer(\n",
    "#     sign_input_dim=sign_ts_dim, sign_cls_dim=sign_cls_dim,\n",
    "#     eeg_input_dim=eeg_ts_dim, eeg_cls_dim=eeg_cls_dim,\n",
    "#     d_model=128, num_classes=1, num_heads=4, num_layers=2,\n",
    "#     sign_max_seq_len=sign_seq_len, eeg_max_seq_len=eeg_seq_len\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(anomaly_model.parameters(), lr=1e-5)\n",
    "# loss_fn = nn.BCEWithLogitsLoss() # only one class, so only one logit output per sample\n",
    "\n",
    "# anomaly_model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     for batch in tqdm(anomaly_train_dataloader, desc=f\"Epoch {epoch+1} Training\", leave=False):\n",
    "#         sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "#         sign_cls_token = batch['sign_cls_token'].to(device)\n",
    "#         sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "#         eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "#         eeg_cls_token = batch['eeg_cls_token'].to(device)\n",
    "#         eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "#         labels = torch.zeros(sign_x_ts.size(0), dtype=torch.float32).to(device)  # All genuine = 0\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         logits = anomaly_model(sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attention_mask, eeg_attention_mask)\n",
    "#         loss = loss_fn(logits.squeeze(1), labels) # logits size turned out to be [8,1] so we need to squeeze it to [8]\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(anomaly_model.parameters(), max_norm=1.0)\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item() * labels.size(0)\n",
    "#     avg_loss = total_loss / len(anomaly_train_dataloader.dataset)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(anomaly_model.state_dict(), os.path.join(os.getenv(\"MODEL_PATH\"), f\"anomaly_detection_model_{datetime.now().strftime('%m%d%Y-%H%M%S')}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # testing loop for anomaly detection\n",
    "# files_mat_test, user_ids_test, test_labels = get_dataset_files_and_user_ids(data_category=constants.ALL, data_type=constants.TEST)\n",
    "# raw_test_data = get_sig_eeg_raw_data(files_mat_test, test_labels)\n",
    "\n",
    "# # Augment and preprocess test data (no augmentation, just features and padding)\n",
    "# for i in range(len(raw_test_data)):\n",
    "#     _, sign_cls_token = get_sign_data_features(raw_test_data[i]['sign_data'])\n",
    "#     _, eeg_cls_token = get_eeg_data_features(raw_test_data[i]['eeg_data'])\n",
    "#     # raw_test_data[i]['sign_data'] = sign_data_with_features\n",
    "#     raw_test_data[i]['sign_cls_token'] = sign_cls_token\n",
    "#     # raw_test_data[i]['eeg_data'] = eeg_data_with_features\n",
    "#     raw_test_data[i]['eeg_cls_token'] = eeg_cls_token\n",
    "\n",
    "# sign_max_seq_len_test = int(max_seq_len_for_data//2)\n",
    "# eeg_max_seq_len_test = int(max_seq_len_for_data // 4)\n",
    "\n",
    "# for i in range(len(raw_test_data)):\n",
    "#     sign_data = raw_test_data[i]['sign_data']\n",
    "#     eeg_data = raw_test_data[i]['eeg_data']\n",
    "#     sign_data, sign_attention_mask = attach_attention_tokens_and_padding(sign_data, sign_max_seq_len_test)\n",
    "#     eeg_data, eeg_attention_mask = attach_attention_tokens_and_padding(eeg_data, eeg_max_seq_len_test)\n",
    "#     raw_test_data[i]['sign_data'] = sign_data\n",
    "#     raw_test_data[i]['eeg_data'] = eeg_data\n",
    "#     raw_test_data[i]['sign_attention_mask'] = sign_attention_mask\n",
    "#     raw_test_data[i]['eeg_attention_mask'] = eeg_attention_mask\n",
    "\n",
    "# test_input_data = {\n",
    "#     'sign_data': [data['sign_data'] for data in raw_test_data],\n",
    "#     'eeg_data': [data['eeg_data'] for data in raw_test_data],\n",
    "#     'sign_attention_masks': [data['sign_attention_mask'] for data in raw_test_data],\n",
    "#     'eeg_attention_masks': [data['eeg_attention_mask'] for data in raw_test_data],\n",
    "#     'sign_cls_tokens': [data['sign_cls_token'] for data in raw_test_data],\n",
    "#     'eeg_cls_tokens': [data['eeg_cls_token'] for data in raw_test_data],\n",
    "#     'labels': [data['label'] for data in raw_test_data],\n",
    "# }\n",
    "\n",
    "# sign_ts_dim = test_input_data['sign_data'][0].size(1)\n",
    "# sign_cls_dim = test_input_data['sign_cls_tokens'][0].size(0)\n",
    "# sign_seq_len = test_input_data['sign_data'][0].size(0)\n",
    "# eeg_ts_dim = test_input_data['eeg_data'][0].size(1)\n",
    "# eeg_cls_dim = test_input_data['eeg_cls_tokens'][0].size(0)\n",
    "# eeg_seq_len = test_input_data['eeg_data'][0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly_test_dataset = SignatureEEGDataset(test_input_data, num_classes=1)\n",
    "# anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.5\n",
    "# anomaly_test_dataset = SignatureEEGDataset(test_input_data, num_classes=1)\n",
    "# anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# anomaly_model.eval()\n",
    "# all_scores = []\n",
    "# all_labels = []\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(anomaly_test_loader, desc=\"Testing\", leave=False):\n",
    "#         sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "#         sign_cls_token = batch['sign_cls_token'].to(device)\n",
    "#         sign_attention_mask = batch['sign_attention_mask'].to(device)\n",
    "#         eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "#         eeg_cls_token = batch['eeg_cls_token'].to(device)\n",
    "#         eeg_attention_mask = batch['eeg_attention_mask'].to(device)\n",
    "#         labels = torch.tensor(batch['labels'], dtype=torch.float32).to(device)\n",
    "#         scores = anomaly_model(sign_x_ts, sign_cls_token, eeg_x_ts, eeg_cls_token, sign_attention_mask, eeg_attention_mask)\n",
    "#         all_scores.extend(scores.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "# preds = (np.array(all_scores) > threshold).astype(int)\n",
    "# print(\"ROC AUC:\", roc_auc_score(all_labels, all_scores))\n",
    "# print(classification_report(all_labels, preds, target_names=['Genuine', 'Anomaly'], labels = [0,1]))\n",
    "# print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53437dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp.pprint(list(zip(all_scores, all_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11217528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.array(all_scores)[np.array(all_labels)==0], bins=50, alpha=0.5, label='Genuine')\n",
    "# plt.hist(np.array(all_scores)[np.array(all_labels)==1], bins=50, alpha=0.5, label='Anomaly')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccedc4",
   "metadata": {},
   "source": [
    "# Sign + EEG in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d8f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureEEGDatasetForCNN(Dataset):\n",
    "    def __init__(self, input_data, num_classes):\n",
    "        sign_data = input_data['sign_data']\n",
    "        eeg_data = input_data['eeg_data']\n",
    "        labels = input_data['labels']\n",
    "\n",
    "        self.sign_x_ts = sign_data\n",
    "\n",
    "        self.eeg_x_ts = eeg_data\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sign_x_ts': self.sign_x_ts[idx],\n",
    "            'eeg_x_ts': self.eeg_x_ts[idx],\n",
    "            'labels': self.labels[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_sign_eeg_for_cnn(input_data):\n",
    "#     sign_data = input_data['sign_data']\n",
    "#     eeg_data = input_data['eeg_data']\n",
    "#     sign_cls_tokens = input_data['sign_cls_tokens']\n",
    "#     eeg_cls_tokens = input_data['eeg_cls_tokens']\n",
    "#     labels = input_data['labels']\n",
    "\n",
    "#     sign_cnn_data = []\n",
    "#     eeg_cnn_data = []\n",
    "\n",
    "#     for s, s_cls, e, e_cls in zip(sign_data, sign_cls_tokens, eeg_data, eeg_cls_tokens):\n",
    "#         # Expand cls token to match sequence length\n",
    "#         s_cls_expanded = s_cls.unsqueeze(0).expand(s.shape[0], -1)\n",
    "#         e_cls_expanded = e_cls.unsqueeze(0).expand(e.shape[0], -1)\n",
    "#         # Concatenate along feature dimension\n",
    "#         sign_cnn_data.append(torch.cat([s, s_cls_expanded], dim=1))\n",
    "#         eeg_cnn_data.append(torch.cat([e, e_cls_expanded], dim=1))\n",
    "\n",
    "#     sign_cnn_data = torch.stack(sign_cnn_data)  # (num_samples, seq_len, sign_ts_dim + sign_cls_dim)\n",
    "#     eeg_cnn_data = torch.stack(eeg_cnn_data)    # (num_samples, seq_len, eeg_ts_dim + eeg_cls_dim)\n",
    "#     labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "#     return sign_cnn_data, eeg_cnn_data, labels\n",
    "\n",
    "# sign_cnn_data, eeg_cnn_data, cnn_labels = prepare_sign_eeg_for_cnn(input_data)\n",
    "# print(\"Sign CNN data shape:\", sign_cnn_data.shape)\n",
    "# print(\"EEG CNN data shape:\", eeg_cnn_data.shape)\n",
    "# print(\"Labels shape:\", cnn_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70236896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignEEGCNN(nn.Module):\n",
    "    def __init__(self, sign_input_dim, eeg_input_dim, num_classes):\n",
    "        super(SignEEGCNN, self).__init__()\n",
    "        self.sign_conv = nn.Sequential(\n",
    "            nn.Conv1d(sign_input_dim, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.eeg_conv = nn.Sequential(\n",
    "            nn.Conv1d(eeg_input_dim, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.sign_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.eeg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64 * 2, num_classes)\n",
    "\n",
    "    def forward(self, sign_x_ts, eeg_x_ts):\n",
    "        sign_x_ts = sign_x_ts.permute(0, 2, 1)\n",
    "        eeg_x_ts = eeg_x_ts.permute(0, 2, 1) \n",
    "\n",
    "        sign_features = self.sign_conv(sign_x_ts)\n",
    "        eeg_features = self.eeg_conv(eeg_x_ts)\n",
    "\n",
    "        sign_features = self.sign_pool(sign_features).squeeze(-1)\n",
    "        eeg_features = self.eeg_pool(eeg_features).squeeze(-1)\n",
    "        combined_features = torch.cat([sign_features, eeg_features], dim=1)\n",
    "\n",
    "        logits = self.fc(combined_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for CNN\n",
    "sign_cnn_data = sign_data\n",
    "eeg_cnn_data = eeg_data\n",
    "cnn_labels = torch.tensor(labels, dtype=torch.long)\n",
    "sign_cnn_data = torch.stack([x if isinstance(x, torch.Tensor) else torch.tensor(x) for x in sign_data])\n",
    "eeg_cnn_data = torch.stack([x if isinstance(x, torch.Tensor) else torch.tensor(x) for x in eeg_data])\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cnn_model = SignEEGCNN(sign_input_dim=sign_cnn_data.shape[2], eeg_input_dim=eeg_cnn_data.shape[2], num_classes=2).to(device)\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "sign_train, sign_test, eeg_train, eeg_test, labels_train, labels_test = train_test_split(\n",
    "    sign_cnn_data, eeg_cnn_data, cnn_labels, test_size=0.2, random_state=42, stratify=cnn_labels\n",
    ")\n",
    "\n",
    "train_dataset = SignatureEEGDatasetForCNN({\n",
    "    'sign_data': sign_train,\n",
    "    'eeg_data': eeg_train,\n",
    "    'labels': labels_train\n",
    "}, num_classes=2)\n",
    "test_dataset = SignatureEEGDatasetForCNN({\n",
    "    'sign_data': sign_test,\n",
    "    'eeg_data': eeg_test,\n",
    "    'labels': labels_test\n",
    "}, num_classes=2)\n",
    "\n",
    "cnn_dataset = SignatureEEGDatasetForCNN({\n",
    "    'sign_data': sign_cnn_data,\n",
    "    'eeg_data': eeg_cnn_data,\n",
    "    'labels': cnn_labels\n",
    "}, num_classes=2)\n",
    "cnn_dataloader = DataLoader(cnn_dataset, batch_size=8, shuffle=True)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    pbar = tqdm(enumerate(cnn_dataloader), total=len(cnn_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch_idx, batch in pbar:\n",
    "        sign_x_ts = batch['sign_x_ts'].to(device)\n",
    "        eeg_x_ts = batch['eeg_x_ts'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = cnn_model(sign_x_ts, eeg_x_ts)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cnn_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(all_labels) if all_labels else 0\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "    print(f\"{'Loss':<10}{'Accuracy':<12}{'Precision':<12}{'Recall':<10}{'F1-Score':<10}\")\n",
    "    print(f\"{avg_loss:<10.4f}{acc:<12.4f}{prec:<12.4f}{rec:<10.4f}{f1:<10.4f}\")\n",
    "    print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c68df6",
   "metadata": {},
   "source": [
    "## EEG - Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008dad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For debugging\n",
    "# def get_max_attention_token_len(attention_tokens):\n",
    "#     max_len = 0\n",
    "#     for item in attention_tokens:\n",
    "#         max_len = max(max_len, item.shape[0])\n",
    "#     return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_max_attention_token_len(eeg_final_dataset['attention_masks']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(normalized_sign_data_dict['000000000200894'][0])\n",
    "# # print(normalized_eeg_data_dict['000000000200894'][0])\n",
    "# # print(user_labels['000000000200894'][0])\n",
    "# eeg_data = get_eeg_data_features(eeg_data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign_data_dict, eeg_data_dict, labels = get_sig_eeg_data_dicts(files_mat_appended, labels_appended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single data\n",
    "# user_id = '002108410100048'\n",
    "# single_eeg_data = {}\n",
    "# single_eeg_data[user_id] = eeg_data_dict[user_id]\n",
    "\n",
    "# normalized_eeg_data_dict = normalize_eeg_data_dict(single_eeg_data)\n",
    "# eeg_data_with_features = get_eeg_data_features(normalized_eeg_data_dict)\n",
    "# eeg_final_data = eeg_attach_attention_tokens_and_labels(eeg_data_with_features, labels)\n",
    "# eeg_final_dataset = prepare_eeg_dataset_with_all_parts(eeg_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c911b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4657fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(eeg_data_with_features['002108410100048']['data'][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec789b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len = get_eeg_max_seq_len(eeg_data_with_features)\n",
    "# print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab796cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(eeg_final_data['002108410100048']['labels'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1eaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(eeg_final_dataset['labels'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff475923",
   "metadata": {},
   "source": [
    "# Misc - Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x1 = torch.tensor([[1.0, 2.0],\n",
    "#                    [3.0, 1.0],\n",
    "#                    [0.0, 0.0]])\n",
    "\n",
    "# x2 = torch.tensor([[2.0, 1.0],\n",
    "#                    [0.0, 3.0],\n",
    "#                    [1.0, 1.0]])\n",
    "# distances = F.pairwise_distance(x1, x2)\n",
    "\n",
    "# print(\"Pairwise distances:\", distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c042f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the paper: The instances corresponding to the last EEG activity for each subject were interpolated to match the length of the longest genuine instance for that subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# class SignatureEEGDataset(Dataset):\n",
    "#     def __init__(self, signature_data, eeg_data, labels):\n",
    "\n",
    "#         self.samples = []\n",
    "        \n",
    "#         # Combine the data\n",
    "#         for user_id in signature_data:\n",
    "#             # Ensure we have matching signature, EEG, and label entries\n",
    "#             n_samples = len(signature_data[user_id])\n",
    "#             for i in range(n_samples):\n",
    "#                 signature = signature_data[user_id][i]\n",
    "#                 eeg = eeg_data[user_id][i]\n",
    "#                 label = labels[user_id][i]\n",
    "                \n",
    "#                 # Convert to tensors\n",
    "#                 signature_tensor = torch.FloatTensor(signature)\n",
    "#                 eeg_tensor = torch.FloatTensor(eeg)\n",
    "#                 label_tensor = torch.LongTensor([label])\n",
    "                \n",
    "#                 self.samples.append((signature_tensor, eeg_tensor, label_tensor))\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.samples[idx]\n",
    "\n",
    "# def collate_fn(batch):\n",
    "    \n",
    "#     signatures, eegs, labels = zip(*batch)\n",
    "    \n",
    "#     # Pad sequences to the same length\n",
    "#     signatures_padded = torch.nn.utils.rnn.pad_sequence(signatures, batch_first=True)\n",
    "#     eegs_padded = torch.nn.utils.rnn.pad_sequence(eegs, batch_first=True)\n",
    "    \n",
    "#     labels = torch.cat(labels)\n",
    "    \n",
    "#     return signatures_padded, eegs_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae257955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SignatureEEGTransformer(nn.Module):\n",
    "#     def __init__(self, signature_dim=7, eeg_channels=10, d_model=128, nhead=4, num_layers=3, num_classes=2):\n",
    "#         super(SignatureEEGTransformer, self).__init__()\n",
    "        \n",
    "#         # Signature embedding\n",
    "#         self.signature_embedding = nn.Linear(signature_dim, d_model)\n",
    "        \n",
    "#         # EEG embedding\n",
    "#         self.eeg_embedding = nn.Linear(eeg_channels, d_model)\n",
    "        \n",
    "#         # Positional encoding\n",
    "#         self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "#         # Transformer encoder\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "#         # Classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(d_model * 2, d_model),  # *2 because we concatenate signature and EEG features\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(d_model, num_classes)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, signature, eeg):\n",
    "#         # Signature processing\n",
    "#         signature_embedded = self.signature_embedding(signature)\n",
    "#         signature_embedded = self.positional_encoding(signature_embedded)\n",
    "        \n",
    "#         # EEG processing\n",
    "#         eeg_embedded = self.eeg_embedding(eeg)\n",
    "#         eeg_embedded = self.positional_encoding(eeg_embedded)\n",
    "        \n",
    "#         # Transformer expects (seq_len, batch, features)\n",
    "#         signature_embedded = signature_embedded.permute(1, 0, 2)\n",
    "#         eeg_embedded = eeg_embedded.permute(1, 0, 2)\n",
    "        \n",
    "#         # Process through transformer\n",
    "#         signature_features = self.transformer_encoder(signature_embedded)\n",
    "#         eeg_features = self.transformer_encoder(eeg_embedded)\n",
    "        \n",
    "#         # Average over time dimension\n",
    "#         signature_features = signature_features.mean(dim=0)\n",
    "#         eeg_features = eeg_features.mean(dim=0)\n",
    "        \n",
    "#         # Concatenate features\n",
    "#         combined_features = torch.cat([signature_features, eeg_features], dim=1)\n",
    "        \n",
    "#         # Classify\n",
    "#         output = self.classifier(combined_features)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(signature_data, eeg_data, labels, epochs=20, batch_size=32):\n",
    "#     # Create dataset\n",
    "#     dataset = SignatureEEGDataset(signature_data, eeg_data, labels)\n",
    "    \n",
    "#     # Split into train and validation\n",
    "#     train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Create dataloaders\n",
    "#     train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "#     val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "#     # Initialize model\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = SignatureEEGTransformer().to(device)\n",
    "    \n",
    "#     # Loss and optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "        \n",
    "#         for signatures, eegs, labels in train_loader:\n",
    "#             signatures, eegs, labels = signatures.to(device), eegs.to(device), labels.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             outputs = model(signatures, eegs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss += loss.item()\n",
    "        \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for signatures, eegs, labels in val_loader:\n",
    "#                 signatures, eegs, labels = signatures.to(device), eegs.to(device), labels.to(device)\n",
    "                \n",
    "#                 outputs = model(signatures, eegs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         print(f'Epoch {epoch+1}/{epochs}')\n",
    "#         print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n",
    "#         print(f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "#         print(f'Val Accuracy: {100*correct/total:.2f}%')\n",
    "#         print('-' * 50)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4996d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train_model(normalized_sign_data_dict, normalized_eeg_data_dict, user_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fa7e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# def prepare_data_with_masking(signature_data, eeg_data, labels_dict):\n",
    "#     \"\"\"\n",
    "#     Prepares data with dynamic padding and preserves original lengths for masking.\n",
    "#     Returns:\n",
    "#         - Padded sequences\n",
    "#         - Sequence length arrays (for masking)\n",
    "#         - Labels\n",
    "#     \"\"\"\n",
    "#     # Initialize lists\n",
    "#     X_signature, X_eeg = [], []\n",
    "#     len_signature, len_eeg = [], []\n",
    "#     y = []\n",
    "    \n",
    "#     for user_id in labels_dict.keys():\n",
    "#         for i in range(len(labels_dict[user_id])):\n",
    "#             # Signature data\n",
    "#             sig = signature_data[user_id][i]\n",
    "#             X_signature.append(sig)\n",
    "#             len_signature.append(len(sig))\n",
    "            \n",
    "#             # EEG data\n",
    "#             eeg = eeg_data[user_id][i]\n",
    "#             X_eeg.append(eeg)\n",
    "#             len_eeg.append(len(eeg))\n",
    "            \n",
    "#             # Label\n",
    "#             y.append(labels_dict[user_id][i])\n",
    "    \n",
    "#     # Pad sequences to max length in dataset\n",
    "#     max_len_sig = max(len_signature)\n",
    "#     max_len_eeg = max(len_eeg)\n",
    "    \n",
    "#     X_signature_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#         X_signature, maxlen=max_len_sig, dtype='float32', padding='post'\n",
    "#     )\n",
    "    \n",
    "#     X_eeg_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#         X_eeg, maxlen=max_len_eeg, dtype='float32', padding='post'\n",
    "#     )\n",
    "    \n",
    "#     return (\n",
    "#         X_signature_pad, np.array(len_signature),\n",
    "#         X_eeg_pad, np.array(len_eeg),\n",
    "#         np.array(y)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0cc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_masked_model(signature_shape, eeg_shape):\n",
    "#     \"\"\"Creates a model with masking layers to ignore padding\"\"\"\n",
    "#     # Signature branch\n",
    "#     signature_input = layers.Input(shape=signature_shape, name='signature_input')\n",
    "#     signature_length = layers.Input(shape=(1,), name='signature_length', dtype='int32')\n",
    "    \n",
    "#     sig = layers.Masking(mask_value=0.0)(signature_input)\n",
    "#     sig = layers.Conv1D(32, 5, activation='relu', padding='same')(sig)\n",
    "#     sig = layers.MaxPooling1D(2)(sig)\n",
    "#     sig = layers.Conv1D(64, 5, activation='relu', padding='same')(sig)\n",
    "#     sig = layers.MaxPooling1D(2)(sig)\n",
    "#     sig = layers.GlobalAveragePooling1D()(sig)\n",
    "    \n",
    "#     # EEG branch\n",
    "#     eeg_input = layers.Input(shape=eeg_shape, name='eeg_input')\n",
    "#     eeg_length = layers.Input(shape=(1,), name='eeg_length', dtype='int32')\n",
    "    \n",
    "#     eeg = layers.Masking(mask_value=0.0)(eeg_input)\n",
    "#     eeg = layers.Conv1D(32, 5, activation='relu', padding='same')(eeg)\n",
    "#     eeg = layers.MaxPooling1D(2)(eeg)\n",
    "#     eeg = layers.Conv1D(64, 5, activation='relu', padding='same')(eeg)\n",
    "#     eeg = layers.MaxPooling1D(2)(eeg)\n",
    "#     eeg = layers.GlobalAveragePooling1D()(eeg)\n",
    "    \n",
    "#     # Combine branches\n",
    "#     combined = layers.concatenate([sig, eeg])\n",
    "#     combined = layers.Dense(128, activation='relu')(combined)\n",
    "#     outputs = layers.Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "#     # Model with length inputs\n",
    "#     model = models.Model(\n",
    "#         inputs=[signature_input, signature_length, eeg_input, eeg_length],\n",
    "#         outputs=outputs\n",
    "#     )\n",
    "    \n",
    "#     model.compile(optimizer='adam',\n",
    "#                  loss='binary_crossentropy',\n",
    "#                  metrics=['accuracy'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X_sig, len_sig, X_eeg, len_eeg, y, batch_size=32):\n",
    "        self.X_sig = X_sig\n",
    "        self.len_sig = len_sig\n",
    "        self.X_eeg = X_eeg\n",
    "        self.len_eeg = len_eeg\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.y) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_sig = self.X_sig[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_len_sig = self.len_sig[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_eeg = self.X_eeg[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_len_eeg = self.len_eeg[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        batch_y = self.y[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        \n",
    "        return [batch_sig, batch_len_sig, batch_eeg, batch_len_eeg], batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ae07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_authentication_model(signature_data, eeg_data, labels_dict):\n",
    "#     # Prepare data\n",
    "#     X_signature, X_eeg, y = prepare_data(signature_data, eeg_data, labels_dict)\n",
    "    \n",
    "#     # Split into train and test sets\n",
    "#     (X_signature_train, X_signature_test, \n",
    "#      X_eeg_train, X_eeg_test, \n",
    "#      y_train, y_test) = train_test_split(X_signature, X_eeg, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     # Create model\n",
    "#     signature_shape = X_signature_train.shape[1:]\n",
    "#     eeg_shape = X_eeg_train.shape[1:]\n",
    "#     model = create_dual_input_model(signature_shape, eeg_shape)\n",
    "    \n",
    "#     # Train model\n",
    "#     history = model.fit(\n",
    "#         [X_signature_train, X_eeg_train],\n",
    "#         y_train,\n",
    "#         epochs=50,\n",
    "#         batch_size=32,\n",
    "#         validation_data=([X_signature_test, X_eeg_test], y_test),\n",
    "#         callbacks=[\n",
    "#             tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "#             tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare data\n",
    "X_sig, len_sig, X_eeg, len_eeg, norm_y = prepare_data_with_masking(signature_data, eeg_data, labels_dict)\n",
    "\n",
    "# 2. Split data\n",
    "(X_sig_train, X_sig_test, \n",
    " len_sig_train, len_sig_test,\n",
    " X_eeg_train, X_eeg_test,\n",
    " len_eeg_train, len_eeg_test,\n",
    " y_train, y_test) = train_test_split(X_sig, len_sig, X_eeg, len_eeg, norm_y, test_size=0.2)\n",
    "\n",
    "# 3. Create model\n",
    "model = create_masked_model(X_sig_train.shape[1:], X_eeg_train.shape[1:])\n",
    "\n",
    "# 4. Create generators\n",
    "train_gen = DataGenerator(X_sig_train, len_sig_train, X_eeg_train, len_eeg_train, y_train)\n",
    "val_gen = DataGenerator(X_sig_test, len_sig_test, X_eeg_test, len_eeg_test, y_test)\n",
    "\n",
    "# 5. Train\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e718c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_signature_test, X_eeg_test, y_test):\n",
    "    results = model.evaluate([X_signature_test, X_eeg_test], y_test)\n",
    "    print(f\"Test Loss: {results[0]:.4f}\")\n",
    "    print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "    print(f\"Test AUC: {results[2]:.4f}\")\n",
    "    \n",
    "    # You can add more evaluation metrics as needed\n",
    "    y_pred = model.predict([X_signature_test, X_eeg_test])\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
