{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc432ad6",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import constants\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.cm as cm\n",
    "import scipy.io as sp\n",
    "import json\n",
    "import pprint as pp\n",
    "from scipy.signal import welch\n",
    "from scipy.integrate import trapezoid\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=300, suppress=True)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9c783",
   "metadata": {},
   "source": [
    "## Data Visualization and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa355f76",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "dataset_path = os.getenv('DATASET_PATH')\n",
    "\n",
    "def get_dataset_files_and_user_ids(data_category = constants.GENUINE, data_type = constants.TRAIN):\n",
    "    user_ids = []\n",
    "    files_csv = []\n",
    "    files_mat = []\n",
    "\n",
    "    # Get training and testing data\n",
    "    data_split = pd.read_csv(os.path.join(dataset_path, \"Identification_split.csv\"))\n",
    "    training_data_files = data_split[data_split.set == constants.TRAIN].filename.str.rsplit('.', n=1).str[0]\n",
    "    # print(training_data_files) # only for debugging\n",
    "\n",
    "    # TODO: get file sbased on type of type required, i.e. Training, tetsing or validation\n",
    "\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        if os.path.basename(root) == constants.GENUINE == data_category:\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    files_csv.append(os.path.join(root, file))\n",
    "                elif file.endswith('.mat'):\n",
    "                    files_mat.append(os.path.join(root, file))\n",
    "        elif os.path.basename(root) == constants.FORGED == data_category:\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    files_csv.append(os.path.join(root, file))\n",
    "                elif file.endswith('.mat'):\n",
    "                    files_mat.append(os.path.join(root, file))\n",
    "        if os.path.basename(root) != constants.GENUINE and os.path.basename(root) != constants.FORGED and os.path.basename(root) != 'SignEEGv1.0':\n",
    "            user_ids.append(os.path.basename(root))\n",
    "    files_csv = sorted(files_csv, key=lambda x: int(x.split('_')[3].split(\".\")[0]))\n",
    "    files_mat = sorted(files_mat, key=lambda x: int(x.split('_')[3]))\n",
    "    return files_csv, files_mat, user_ids\n",
    "\n",
    "# print(\"Genuine MAT files:\")\n",
    "# pprint(get_genuine_csv_mat_files())\n",
    "# print(\"Forged MAT files:\")\n",
    "# pprint(get_forged_csv_mat_files())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ff10b",
   "metadata": {},
   "source": [
    "### Get List of UserIDs from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ids = []\n",
    "# def get_list_of_user_ids():\n",
    "#     for root, dir, files in os.walk(dataset_path):\n",
    "#         if os.path.basename(root) != 'Genuine' and os.path.basename(root) != 'Forged' and os.path.basename(root) != 'SignEEGv1.0':\n",
    "#             user_ids.append(os.path.basename(root))\n",
    "#     # print(len(user_ids))\n",
    "#     return user_ids\n",
    "\n",
    "# print(\"User IDs:\")\n",
    "# pprint(get_list_of_user_ids())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe5a53",
   "metadata": {},
   "source": [
    "### Clean up Signature CSV data and reset column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e500c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_csv_sign_data_cleaned(user_sign_data_csv): #Provide file name of the csv file\n",
    "    content = pd.read_csv(user_sign_data_csv, skiprows=1, header=None)\n",
    "    content.drop\n",
    "    content.columns = [c.strip() for c in content.iloc[0]] #gettting rid of extra space in column names\n",
    "    content = content.iloc[1:]\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f98fc39",
   "metadata": {},
   "source": [
    "### Plot signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cca9f",
   "metadata": {},
   "source": [
    "#### Uncomment in case images of signatures need to be generated again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# csv_data, _ = get_genuine_csv_mat_files()\n",
    "# print(csv_data)\n",
    "# user_id = '000000001045402'\n",
    "# user_match = [data for data in csv_data if user_id in data]\n",
    "# print(\"Users matched with user_id '{}':\".format(user_match))\n",
    "# count=0\n",
    "# for file in user_match:\n",
    "#     count+=1\n",
    "#     content = get_user_csv_sign_data_cleaned(file)\n",
    "#     x, y, _, press, _, _ = normalize_sign_data(content)\n",
    "#     # cmap = cm.Blues\n",
    "#     # colors = cmap(press)\n",
    "#     # # colors[:, 3] = press\n",
    "#     # # plt.scatter(sign_coords['X'], sign_coords['Y'], c=colors, s=50)\n",
    "#     # plt.scatter(x, y, c=colors, s=50)\n",
    "#     # plt.title('Sign Coordinates')\n",
    "#     # plt.xlabel('X')\n",
    "#     # plt.ylabel('Y')\n",
    "#     # plt.show()\n",
    "\n",
    "#     min_linewidth = 0\n",
    "#     max_linewidth = 5.0\n",
    "#     linewidths = min_linewidth + press * (max_linewidth - min_linewidth)\n",
    "\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     for i in range(len(x) - 1):\n",
    "#         plt.plot(\n",
    "#             x[i:i+2], y[i:i+2],\n",
    "#             linewidth=linewidths[i],\n",
    "#             color='black',\n",
    "#             solid_capstyle='round'\n",
    "#         )\n",
    "#     # plt.axis('equal')  # Keep aspect ratio square\n",
    "#     plt.axis('off')    # Hide axes for cleaner look\n",
    "#     # plt.show() # uncomment only for debug processes, uncommenting will make plt.savefigure() save blank images\n",
    "\n",
    "#     plt.savefig(\"SignImages\\\\\"+user_id+\"-\"+str(count)+\".jpeg\", dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11db1a",
   "metadata": {},
   "source": [
    "### Signature Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caaef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sign_data(data):\n",
    "    x = np.array(data['X']).astype(int)\n",
    "    y = np.array(data['Y']).astype(int)\n",
    "    t = np.array(data['T']).astype(int)\n",
    "    pressure = np.array(data['Pressure']).astype(int)\n",
    "    azimuth = np.array(data['Azimuth']).astype(int)\n",
    "    altitude = np.array(data['Altitude']).astype(int)\n",
    "    # normalize signature data\n",
    "    norm_x = x / np.max(x)\n",
    "    norm_y = y / np.max(y)\n",
    "    norm_pressure = pressure / np.max(pressure)\n",
    "    norm_azimuth = azimuth / np.max(azimuth)\n",
    "    norm_altitude = altitude / np.max(altitude)\n",
    "    return norm_x, norm_y, t, norm_pressure, norm_azimuth, norm_altitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ea280",
   "metadata": {},
   "source": [
    "### EEG Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mat_data, user_ids = get_dataset_files_and_user_ids()\n",
    "# print(mat_data)\n",
    "def get_user_mat_data(user_id=None):\n",
    "    if user_id is None:\n",
    "        user_id = user_ids[0]  # Default to the first user if none specified\n",
    "    user_files = [x for x in mat_data if user_id in x]\n",
    "    user_files_sorted = pd.Series(user_files)\n",
    "    user_files_sorted.sort_values(key=lambda x: x.str.split('_').str[3].astype(int), inplace=True)\n",
    "    user_files_reset = user_files_sorted.reset_index(drop=True)\n",
    "    # print(user_files_reset)\n",
    "    return user_files_reset\n",
    "\n",
    "# Fetch matlab data\n",
    "mat_files_sorted = get_user_mat_data()\n",
    "mat_content = sp.loadmat(mat_files_sorted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71dff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing using z-score\n",
    "\n",
    "def normalize_eeg_data(eeg_input):\n",
    "    norm_eeg_data = []  \n",
    "    for channel in eeg_input:\n",
    "        mean = np.mean(channel)\n",
    "        std = np.std(channel)\n",
    "        norm_channel = (channel - mean)/std\n",
    "        norm_eeg_data.append(norm_channel)\n",
    "    norm_eeg_array = np.array(norm_eeg_data)\n",
    "    # print(norm_eeg_data)\n",
    "    return norm_eeg_array\n",
    "\n",
    "# normalize_eeg_data(eeg_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a03aab",
   "metadata": {},
   "source": [
    "### EEG Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eeg_data(eeg_data):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    offset = 500\n",
    "    colors = ['b', 'g', 'r', 'c', 'y']\n",
    "    for idx, col in enumerate(eeg_columns):\n",
    "        plt.plot(eeg_data[col] + idx * offset, color=colors[idx % len(colors)], label=col)\n",
    "    plt.title('EEG Signal Data (with vertical offset)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude + Offset')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca78353",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38189dbe",
   "metadata": {},
   "source": [
    "### Signature data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4db187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign_data = get_user_csv_sign_data_cleaned('D:\\\\KCL Final Year Individual Project\\\\Implementation\\\\Project Implementation\\\\Dataset\\\\SignEEGv1.0\\\\SignEEGv1.0\\\\000000000200894\\\\Genuine\\\\000000000200894_Genuine_000000000200894_1.csv')\n",
    "# sign_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de594ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data, mat_data, user_ids = get_dataset_files_and_user_ids()\n",
    "def get_signature_feature_vector(path, seq_len = 256, overlap = 0.5):\n",
    "    user_id = [id for id in user_ids if id in path][0]\n",
    "    sign_data = get_user_csv_sign_data_cleaned(path)\n",
    "    x, y, t, pressure, azimuth, altitude = normalize_sign_data(sign_data)\n",
    "\n",
    "    # Calculate pen velocity\n",
    "    dt = 1 / (4 / 1000)\n",
    "    vx = np.gradient(x, dt)\n",
    "    vy = np.gradient(y, dt)\n",
    "    v = np.sqrt(vx**2 + vy**2)\n",
    "    \n",
    "    # Calculate pen acceleration\n",
    "    ax = np.gradient(vx, dt)\n",
    "    ay = np.gradient(vy, dt)\n",
    "    a = np.sqrt(ax**2 + ay**2)\n",
    "\n",
    "    # Calculate number of pen lifts\n",
    "    # Do a logical & betwen the values of the array(except for the last) are > 0 and the values for which (except the first element) > 0\n",
    "    pen_lifts = np.sum((pressure[:-1] > 0) & (pressure[1:] == 0))\n",
    "    # print(pen_lifts)\n",
    "\n",
    "    # Calculate stroke duration\n",
    "    is_pen_down = pressure > 0 \n",
    "    stroke_durations = []\n",
    "    start = None\n",
    "    stroke_count = 0\n",
    "\n",
    "    for i in range(len(pressure)):\n",
    "        if is_pen_down[i]:\n",
    "            if start is None:\n",
    "                start = i\n",
    "        else:\n",
    "            if start is not None:\n",
    "                duration = t[i-1] - t[start]\n",
    "                stroke_durations.append(int(duration))\n",
    "                start = None\n",
    "\n",
    "    # Handle case where the last stroke goes to the end\n",
    "    if start is not None:\n",
    "        duration = t[-1] - t[start]\n",
    "        stroke_durations.append(int(duration))\n",
    "    stroke_durations = np.array(stroke_durations)\n",
    "    # Calculate average stroke duration\n",
    "    avg_stroke_duration = np.average(stroke_durations)\n",
    "    \n",
    "    # Calculate number of strokes\n",
    "    stroke_count = len(stroke_durations)\n",
    "\n",
    "    # Sign centroid\n",
    "    pen_down = pressure > 0\n",
    "    x_down = x[pen_down]\n",
    "    y_down = y[pen_down]\n",
    "    centroid_x = np.mean(x_down)\n",
    "    centroid_y = np.mean(y_down)\n",
    "    sign_centroid = np.array([centroid_x, centroid_y])\n",
    "\n",
    "    sign_centroid, [pen_lifts], [stroke_count], [avg_stroke_duration], stroke_durations\n",
    "    # convert to array of shape (num_frames, num_features)\n",
    "    summary_features = np.zeros((1, 7))\n",
    "    summary_features[0, :2] = sign_centroid\n",
    "    summary_features[0, 4:7] = [pen_lifts, stroke_count, avg_stroke_duration]\n",
    "    sign_feature_data = np.stack([x, y, pressure, azimuth, altitude, v, a], axis = 1)\n",
    "    sign_feature_data = np.vstack([summary_features, sign_feature_data])\n",
    "\n",
    "    # Convert to sliding window as a tensor for input to transformer model\n",
    "    # cls token - added to let the transformer know it's a classification task. will add it to every sliding window.\n",
    "    cls_token = sign_feature_data[0]\n",
    "    feature_data_for_sign = sign_feature_data[1:]\n",
    "    full_len = feature_data_for_sign.shape[0]\n",
    "    stride = int(seq_len * (1 - overlap))\n",
    "\n",
    "    sign_vector_with_windows = []\n",
    "\n",
    "    for start in range(0, full_len, stride):\n",
    "        end = start + seq_len - 1\n",
    "        if start >= full_len:\n",
    "            break\n",
    "        sliding_win = feature_data_for_sign[start:end]\n",
    "\n",
    "        # sliding win size <256 -1; -1 because we need to add the cls token as well\n",
    "        if sliding_win.shape[0] < seq_len - 1:\n",
    "            padding_len = seq_len - 1 - sliding_win.shape[0]\n",
    "            padding = np.zeros((padding_len, feature_data_for_sign.shape[1]))\n",
    "            sliding_win = np.vstack([sliding_win, padding])\n",
    "\n",
    "        # sliding_win_tensor = torch.tensor(sliding_win, dtype = torch.float32)\n",
    "        # cls_tensor = torch.tensor(cls_token, dtype = torch.float32)\n",
    "        sliding_win = np.vstack([cls_token, sliding_win])\n",
    "\n",
    "        # Create attention mask, to filter out padding when feeding to transformer\n",
    "        data_len = min(seq_len - 1, full_len - start)\n",
    "        attention_mask = torch.tensor([1] + [1] * data_len + [0] * (seq_len - 1 - full_len))\n",
    "        sliding_win = torch.tensor(sliding_win)\n",
    "        sign_vector_with_windows.append([sliding_win, attention_mask])\n",
    "        if end >= full_len:\n",
    "            break\n",
    "    \n",
    "    return sign_vector_with_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ab844",
   "metadata": {},
   "source": [
    "#### Misc - for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign_feature_vector = get_signature_feature_vector(csv_data[0])\n",
    "# print(sign_feature_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_data_roi = eeg_input[roi_idx[0]:roi_idx[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(eeg_data_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mat_files_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285cb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_eeg_data(eeg_data_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43701b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pp.pprint(eeg_data)\n",
    "# Side note: Can be used for sign data as well, reduces dependency on CSV data\n",
    "# plot_eeg_data(eeg_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ee375",
   "metadata": {},
   "source": [
    "### Extract EEG Frequency Weighted Power Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eaa8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def modify_sign_data_for_transformer(sign_vector, seq_len = constants.sign_seq_len, overlap = 0.5):\n",
    "    \n",
    "#     # Putting overlap here to give transition to each sliding window\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# To complete\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135009ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_power_spectral_density(norm_eeg_signal, sampling_freq):\n",
    "#     freqs, psd = welch(norm_eeg_signal, fs = sampling_freq, nperseg = sampling_freq * 2) # window = 'hann' by default\n",
    "#     # print(\"Frequencies: \", freqs)\n",
    "#     # print(\"Power distribution: \", psd)\n",
    "\n",
    "#     band_psd = {}\n",
    "#     for band, [low, high] in freq_bands.items():\n",
    "#         idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "#         band_psd[band] = np.mean(psd[:, idx_band], axis = 1) if psd[:, idx_band].size > 0 else np.zeros(norm_eeg_signal.shape[0])\n",
    "#     print(\"Band Powers: \", band_psd)\n",
    "#     return band_psd\n",
    "# psd_data = calculate_power_spectral_density(norm_eeg_data, 128)\n",
    "\n",
    "def normalize_for_eeg_related_data(data):\n",
    "    mean = np.mean(data, axis = 0)\n",
    "    std = np.std(data, axis = 0)\n",
    "    std[std == 0] = 1\n",
    "    norm = (data - mean) / std\n",
    "    return norm\n",
    "\n",
    "def get_nth_difference_mean_for_signal(input_signal, n):\n",
    "    diff = np.abs(input_signal[n:] - input_signal[:-n])\n",
    "    res = np.sum(diff) / (input_signal.shape[0] - n)\n",
    "    return res\n",
    "\n",
    "def compute_freq_weighted_power_per_channel(channel, samp_freq, band):\n",
    "    freqs, psd = welch(channel, fs=samp_freq, nperseg=len(channel))\n",
    "    idx = (freqs >= band[0]) & (freqs <= band[1])\n",
    "    freqs = freqs[idx]\n",
    "    psd = psd[idx]\n",
    "    return np.sum(freqs * psd) / np.sum(psd) if np.sum(psd) > 0 else 0\n",
    "\n",
    "def get_freq_weighted_feature(signal, samp_freq, seq_len = 64, window = 2, overlap = 0.5, normalize = False):\n",
    "\n",
    "    # over different frequency bands, calculate power\n",
    "    # standard bands used for EEG - gamma (20-50 Hz), beta (13-20 Hz), alpha (8-13 Hz), theta (4-8 Hz), delta (0.5-4 Hz)\n",
    "    # also takign windows of 2seconds witgh 1 second overlap\n",
    "    # windows made using Hann window\n",
    "    freq_bands = {\n",
    "        'delta': [0.5, 4],\n",
    "        'theta': [4, 8],\n",
    "        'alpha': [8, 13],\n",
    "        'beta': [13, 20],\n",
    "        'gamma': [20, 50]\n",
    "    }\n",
    "    \n",
    "    # Extract raw signal statistical features\n",
    "    signal_mean = np.mean(np.array([np.mean(x) for x in signal]))\n",
    "    signal_std = np.std(np.array(signal))\n",
    "    \n",
    "    \n",
    "    first_difference_sample_mean_absolute_difference_raw_signal = get_nth_difference_mean_for_signal(signal, 1)\n",
    "    second_difference_sample_mean_absolute_difference_raw_signal = get_nth_difference_mean_for_signal(signal, 2)\n",
    "    \n",
    "    normalized_signal = normalize_for_eeg_related_data(signal)\n",
    "    first_difference_sample_mean_absolute_difference_normalized_signal = get_nth_difference_mean_for_signal(normalized_signal, 1)\n",
    "    second_difference_sample_mean_absolute_difference_normalized_signal = get_nth_difference_mean_for_signal(normalized_signal, 2)\n",
    "\n",
    "\n",
    "    n_channels, n_samples = signal.shape\n",
    "    window_len = int(samp_freq * window)\n",
    "    step = int(samp_freq * overlap)\n",
    "    n_windows = (n_samples - window_len) // step + 1\n",
    "\n",
    "    features = []\n",
    "    for w in range(n_windows):\n",
    "        start = w * step\n",
    "        end = start + window_len\n",
    "        window_features = []\n",
    "        for channel in range(n_channels):\n",
    "            segment = signal[channel, start:end]\n",
    "            freqs, psd = welch(segment, fs = samp_freq, nperseg = window_len)\n",
    "            for band_range in freq_bands.values():\n",
    "                idx = (freqs >= band_range[0]) & (freqs <= band_range[1])\n",
    "                bp = trapezoid(psd[idx], freqs[idx])\n",
    "                window_features.append(bp)\n",
    "            fwp = compute_freq_weighted_power_per_channel(segment, samp_freq=samp_freq, band = (0.5, 50))\n",
    "            window_features.append(fwp)\n",
    "        features.append(window_features)\n",
    "    features = np.array(features)\n",
    "    # print(\"Features shape before normalizing: \", features.shape)\n",
    "    if normalize:\n",
    "        features = normalize_for_eeg_related_data(features)\n",
    "    # print(features.shape)\n",
    "\n",
    "    cls_token = np.zeros(features.shape[1])\n",
    "    cls_token[:6] = [signal_mean, signal_std, first_difference_sample_mean_absolute_difference_raw_signal, second_difference_sample_mean_absolute_difference_raw_signal, first_difference_sample_mean_absolute_difference_normalized_signal, second_difference_sample_mean_absolute_difference_normalized_signal]\n",
    "    features = np.vstack([cls_token, features])\n",
    "    return features\n",
    "\n",
    "# def pad_eeg_signal_for_transformer(eeg_feature_vector, seq_len = constants.eeg_seq_len):\n",
    "    \n",
    "#     print(eeg_feature_vector.shape)\n",
    "#     # max_len = max([feature.shape[0] for feature in eeg_feature_vector])\n",
    "#     for feature in eeg_feature_vector:\n",
    "#         print(feature.shape)\n",
    "#         cls_token = feature[0]\n",
    "        \n",
    "\n",
    "# print(get_freq_weighted_feature(norm_eeg_data, 128, normalize = True))\n",
    "\n",
    "def modify_eeg_feature_data_for_transformer(eeg_feature_data):\n",
    "\n",
    "    eeg_max_window = max([item[0].shape[0] for item in eeg_feature_data.values()])\n",
    "    eeg_feature_vector_with_padding_and_attention = {}\n",
    "    # print(\"Maximum n_windows found for EEG: \", eeg_max_window)\n",
    "    for key, value in eeg_feature_data.items():\n",
    "        # print(\"UserID: \", key)\n",
    "        # print(\"Feature size: \", value[0].shape)\n",
    "        current_user_eeg_feature_data = []\n",
    "        for feature in value:\n",
    "            # print(\"Feature: \", feature.shape)\n",
    "            eeg_padded = feature\n",
    "            if feature.shape[0] < eeg_max_window:\n",
    "                padding = np.zeros((eeg_max_window - feature.shape[0], feature.shape[1]))\n",
    "                # eeg_padded = torch.cat([feature, padding])\n",
    "                eeg_padded = np.vstack([feature, padding])\n",
    "            # The attention mask should have 1s for real tokens (including the cls_token) and 0s for padding\n",
    "            eeg_padded = torch.tensor(eeg_padded)\n",
    "            real_len = feature.shape[0]\n",
    "            mask = np.array([1] * real_len + [0] * (eeg_max_window - real_len))\n",
    "            attention_mask = torch.tensor(mask, dtype = torch.int64)\n",
    "            current_user_eeg_feature_data.append([eeg_padded, attention_mask])\n",
    "            # attention_mask = mask\n",
    "        eeg_feature_vector_with_padding_and_attention[key] = current_user_eeg_feature_data\n",
    "    return eeg_feature_vector_with_padding_and_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_eeg_features(mat_file):\n",
    "\n",
    "    # mat_files_sorted = get_user_mat_data(user_id)\n",
    "    mat_content = sp.loadmat(mat_file)\n",
    "    \n",
    "    # For debugging issues\n",
    "    # to_print = mat_content['subject']\n",
    "    # print(to_print)\n",
    "\n",
    "    # converting ICA_EEG data into np structured array\n",
    "    # eeg_columns = [i for i in mat_content['subject']['EEGHeader'][0][0][0].split(\", \")]\n",
    "    eeg_data_list = [i.tolist() for i in mat_content['subject']['ICA_EEG'][0][0]]\n",
    "\n",
    "    # print(eeg_data_list)\n",
    "    # eeg_input = pd.DataFrame(eeg_data_list).T\n",
    "    # eeg_input.columns = eeg_columns\n",
    "\n",
    "    norm_eeg_data = normalize_eeg_data(eeg_data_list)\n",
    "    \n",
    "    eeg_features_vector = get_freq_weighted_feature(norm_eeg_data, constants.eeg_samp_freq, normalize = True)\n",
    "    return eeg_features_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f31196",
   "metadata": {},
   "source": [
    "#### Misc - for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99134257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eeg_feature_vector = get_eeg_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sign_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(eeg_feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c6f54",
   "metadata": {},
   "source": [
    "### Modify feature vectors for giving as input to Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ba5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vectors_for_all_users():\n",
    "    csv_files, mat_files, user_ids = get_dataset_files_and_user_ids()\n",
    "    sign_features_for_all_users = {}\n",
    "    eeg_features_for_all_users = {}\n",
    "    for user in user_ids:\n",
    "        sign_features_for_all_users[user] = []\n",
    "        eeg_features_for_all_users[user] = []\n",
    "        user_csv_raw = [file for file in csv_files if user in file]\n",
    "        user_mat_raw = [file for file in mat_files if user in file]\n",
    "        # for debugging only\n",
    "        print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"User ID: \", user)\n",
    "\n",
    "        # Uncomment for debugging purposes only\n",
    "        # print(\"CSV Data: \")\n",
    "        # pp.pprint(user_csv_raw)\n",
    "        # print(\"\\n\")\n",
    "        # print(\"MAT Data:\")\n",
    "        # pp.pprint(user_mat_raw)\n",
    "\n",
    "        for csv_file in user_csv_raw:\n",
    "            sign_feature_vector = get_signature_feature_vector(csv_file)\n",
    "            # sign_feature_vector = modify_sign_data_for_transformer(sign_feature_vector)\n",
    "            print(\"Extracting sign features for file: \", csv_file)\n",
    "            # print(\"Sign feature vector: \")\n",
    "            # pp.pprint(sign_feature_vector)\n",
    "            sign_features_for_all_users[user].append(sign_feature_vector)\n",
    "\n",
    "        for mat_file in user_mat_raw:\n",
    "            eeg_feature_vector = get_eeg_features(mat_file)\n",
    "            # eeg_feature_vector = pad_eeg_signal_for_transformer(eeg_feature_vector)\n",
    "            print(\"Extracting EEG features for file: \", mat_file)\n",
    "            eeg_features_for_all_users[user].append(eeg_feature_vector)\n",
    "    eeg_features_for_all_users = modify_eeg_feature_data_for_transformer(eeg_features_for_all_users)\n",
    "    return sign_features_for_all_users, eeg_features_for_all_users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ba210",
   "metadata": {},
   "source": [
    "### Single User Run (Run only for Debugging/Testing/Presentation purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99616352",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "\n",
    "csv_files, mat_files, user_ids = get_dataset_files_and_user_ids()\n",
    "sign_features_for_all_users = {}\n",
    "eeg_features_for_all_users = {}\n",
    "user = user_ids[1]\n",
    "sign_features_for_all_users[user] = []\n",
    "eeg_features_for_all_users[user] = []\n",
    "user_csv_raw = [file for file in csv_files if user in file]\n",
    "user_mat_raw = [file for file in mat_files if user in file]\n",
    "# Set a global state for tracking maximum number of windows in eeg vectors\n",
    "\n",
    "global eeg_max_window\n",
    "eeg_max_window = 0\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"User ID: \", user)\n",
    "for csv_file in user_csv_raw:\n",
    "    sign_feature_vector = get_signature_feature_vector(csv_file)\n",
    "    # sign_feature_vector = modify_sign_data_for_transformer(sign_feature_vector)\n",
    "    print(\"Extracting sign features for file: \", csv_file)\n",
    "    sign_features_for_all_users[user].append(sign_feature_vector)\n",
    "    print(f\"Signature Feature Vector shape: {sign_feature_vector[-1][0].shape}\")\n",
    "for mat_file in user_mat_raw:\n",
    "    eeg_feature_vector = get_eeg_features(mat_file)\n",
    "    print(\"Extracting EEG features for file: \", mat_file)\n",
    "    eeg_features_for_all_users[user].append(eeg_feature_vector)\n",
    "    print(f\"EEG Feature Vector shape: {eeg_feature_vector.shape}\")\n",
    "eeg_features_for_all_users = modify_eeg_feature_data_for_transformer(eeg_features_for_all_users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_features_final, eeg_features_final = get_feature_vectors_for_all_users()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db49979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eeg_features_final[\"000000000200894\"][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60797434",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52748e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [item[0] for item in eeg_features_final.items(\n",
    "[item[1].shape for item in eeg_features_final['002108410100008']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ef915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(item) for item in eeg_features_final.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4beb47d",
   "metadata": {},
   "source": [
    "### Create batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = []\n",
    "user_ids\n",
    "for user_id in user_ids:\n",
    "    sign_features_all = sign_features_final[user_id]\n",
    "    eeg_features_all = eeg_features_final[user_id]\n",
    "    sign_features_concatenated\n",
    "    print(\"Signature features length: \", sign_features_all[0][0][0].shape)\n",
    "    print(\"EEG features length: \", eeg_features_all[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f121599",
   "metadata": {},
   "source": [
    "## Creating Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b41898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, d_model)\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask expected shape: (batch_size, seq_len)\n",
    "            # TransformerEncoder expects mask shape (seq_len, seq_len), so convert mask to key_padding_mask\n",
    "            key_padding_mask = ~attn_mask.bool()  # True where to mask\n",
    "        else:\n",
    "            key_padding_mask = None\n",
    "        out = self.transformer_encoder(x, src_key_padding_mask=key_padding_mask)  # (batch_size, seq_len, d_model)\n",
    "        return out\n",
    "\n",
    "class MultiModalAuthModel(nn.Module):\n",
    "    def __init__(self, sig_input_dim, eeg_input_dim, n_subjects,\n",
    "                 d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sig_transformer = SimpleTransformerEncoder(sig_input_dim, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "        self.eeg_transformer = SimpleTransformerEncoder(eeg_input_dim, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "\n",
    "        # Classification head\n",
    "        # CLS tokens embedding concatenated (2 * d_model)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, n_subjects)\n",
    "        )\n",
    "\n",
    "    def forward(self, sig_x, sig_mask, eeg_x, eeg_mask):\n",
    "        # sig_x: (batch, seq_len_sig, sig_feat_dim)\n",
    "        # sig_mask: (batch, seq_len_sig) - 1 where data, 0 where padding\n",
    "        # eeg_x: (batch, seq_len_eeg, eeg_feat_dim)\n",
    "        # eeg_mask: (batch, seq_len_eeg)\n",
    "        \n",
    "        sig_out = self.sig_transformer(sig_x, sig_mask)  # (batch, seq_len_sig, d_model)\n",
    "        eeg_out = self.eeg_transformer(eeg_x, eeg_mask)  # (batch, seq_len_eeg, d_model)\n",
    "\n",
    "        # Extract CLS token embedding (assumed at index 0)\n",
    "        sig_cls = sig_out[:, 0, :]  # (batch, d_model)\n",
    "        eeg_cls = eeg_out[:, 0, :]  # (batch, d_model)\n",
    "\n",
    "        combined = torch.cat([sig_cls, eeg_cls], dim=-1)  # (batch, 2*d_model)\n",
    "        logits = self.classifier(combined)  # (batch, n_subjects)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af856c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalAuthModel(sig_input_dim=8, eeg_input_dim=30, n_subjects=70)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        sig_x, sig_mask, eeg_x, eeg_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(sig_x, sig_mask, eeg_x, eeg_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
